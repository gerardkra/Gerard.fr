}
liste=c()
x=0
for (k in 1:n)
{
liste=c(liste,x)
v=runif(1,0,1)
if (v<0.5)
{ y=x+1 }
else
{ y=x-1 }
u=runif(1,0,1)
alpha=min(1,f(y)*0.5/(f(x)*0.5))
if (u<alpha)
{ x=y }
}
View(f)
al=1/10 s=0
al=1/10s=0
al=1/10 , s=0
al=1/10 s=0
# Générer une série temporelle x de longueur 100 (à des fins de démonstration)
set.seed(123)
x <- rnorm(100)
# Initialiser une liste de valeurs pour α
alpha_list <- seq(0.1, 0.9, by = 0.1)
# Fonction pour calculer la somme des carrés des erreurs pour un α donné
calculate_SE <- function(alpha, series) {
predictions <- HoltWinters(series, alpha = alpha, beta = alpha)$fitted[, "fitted"]
errors <- series[51:100] - predictions
sum(errors^2)
}
# Initialiser la somme des carrés des erreurs minimale et la valeur correspondante de α
min_SE <- Inf
best_alpha <- NULL
# Boucle for pour itérer sur les valeurs de α
for (alpha in alpha_list) {
current_SE <- calculate_SE(alpha, x)
# Mettre à jour si la somme des carrés des erreurs actuelle est plus petite
if (current_SE < min_SE) {
min_SE <- current_SE
best_alpha <- alpha
}
}
# Générer une série temporelle x de longueur 100 (à des fins #de démonstration)
set.seed(123)
x <- rnorm(100)
# Initialiser une liste de valeurs pour α
#alpha_list <- seq(0.1, 0.9, by = 0.1)
# Fonction pour calculer la somme des carrés des erreurs pour #un α donné
calculate_SE <- function(alpha, series) {
predictions <- HoltWinters(series, alpha = alpha, beta = alpha)$fitted[, "fitted"]
errors <- series[51:100] - predictions
sum(errors^2)
}
# Initialiser la somme des carrés des erreurs minimale et la #valeur correspondante de α
min_SE <- Inf
best_alpha <- NULL
# Boucle for pour itérer sur les valeurs de α
for (alpha in alpha_list) {
current_SE <- calculate_SE(alpha, x)
# Mettre à jour si la somme des carrés des erreurs actuelle est plus petite
if (current_SE < min_SE) {
min_SE <- current_SE
best_alpha <- alpha
}
}
# Chargement de la base de données
data(lynx)
head(lynx)
plot(lynx)
acf(lynx, lag.max = 30,type = c("correlation"))
acf(lynx, lag.max = 30,type = c("correlation"))
pacf(lynx, lag.max = 30,type = c("correlation"))
acf(lynx, lag.max = 30,type = c("correlation"))
pacf(lynx, lag.max = 30)
# Chargement de la base de données
data(lynx)
head(lynx)
plot(lynx)
acf(lynx, lag.max = 30,type = c("correlation"))
pacf(lynx, lag.max = 30)
TT <- 100
wn <- rnorm(TT)
adf.test(wn)
library(tseries)
library(tseries)
TT <- 100
wn <- rnorm(TT)
adf.test(wn)
acf(wn,lag=50)
pacf(wn,lag=50)
acf(wn,lag=100)
pacf(wn,lag=100)
acf(wn,lag=25)
pacf(wn,lag=25)
wnt<-wn
for ( i in 4:TT)
{
wnt[i]=wnt[i-1]/4+2*wnt[i-2]/4+wnt[i-3]/4+wn[i]
}
plot(wnt)
wnt<-wn
for ( i in 4:TT)
{
wnt[i]=wnt[i-1]/4+2*wnt[i-2]/4+wnt[i-3]/4+wn[i]
}
plot(wnt)
adf.test(wnt)
# Installer la bibliothèque class si ce n'est pas déjà fait
# install.packages("class")
# Charger la bibliothèque class
library(class)
# Fonction k-NN
knn_function <- function(training_data, test_data, k = 3) {
# Extrait les colonnes avec les caractéristiques (variables prédictives)
train_features <- training_data[, -ncol(training_data)]
test_features <- test_data[, -ncol(test_data)]
# Extrait les colonnes avec les étiquettes (réponses)
train_labels <- training_data[, ncol(training_data)]
# Applique la méthode des k-plus-proches-voisins
predictions <- knn(train = train_features, test = test_features, cl = train_labels, k = k)
return(predictions)
}
# Exemple d'utilisation
# Créer des données d'entraînement et de test fictives
set.seed(123)
train_data <- data.frame(x1 = rnorm(20), x2 = rnorm(20), y = c(rep("A", 10), rep("B", 10)))
test_data <- data.frame(x1 = rnorm(5), x2 = rnorm(5))
# Appeler la fonction k-NN
predictions <- knn_function(training_data = train_data, test_data = test_data, k = 3)
# Fonction k-NN
knn <- function(train_data, test_data, k = 3) {
distances <- sqrt(rowSums((train_data - test_data)^2))
sorted_indices <- order(distances)
k_nearest_neighbors <- sorted_indices[1:k]
return(k_nearest_neighbors)
}
# Exemple avec une variable aléatoire de loi normale
set.seed(123)  # Pour la reproductibilité
train_data <- rnorm(100, mean = 0, sd = 1)  # Données d'entraînement
test_data <- rnorm(1, mean = 2, sd = 1)  # Donnée de test
# Appliquer la fonction k-NN
k_nearest_indices <- knn(train_data, test_data, k = 5)
# Fonction k-NN
k_nn <- function(data, k, x) {
# Calculer la distance euclidienne entre x et chaque point dans le jeu de données
distances <- sqrt(rowSums((data[, -ncol(data)] - x)^2))
# Trier les indices en fonction de la distance
sorted_indices <- order(distances)
# Sélectionner les k plus proches voisins
k_nearest_indices <- sorted_indices[1:k]
# Récupérer les labels des k plus proches voisins
k_nearest_labels <- data[k_nearest_indices, ncol(data)]
# Vérifier la condition classique pour le cas binaire
if (length(unique(k_nearest_labels)) == 1) {
class_label <- unique(k_nearest_labels)
} else {
# En cas d'égalité, on peut utiliser la majorité ou une autre règle de décision
class_label <- names(sort(table(k_nearest_labels), decreasing = TRUE))[1]
}
# Retourner les indices des k plus proches voisins et le label prédit
return(list(nearest_indices = k_nearest_indices, predicted_label = class_label))
}
# Exemple d'utilisation avec une variable aléatoire de loi normale
set.seed(123)
data <- data.frame(X1 = rnorm(100), X2 = rnorm(100), label = sample(c(0, 1), 100, replace = TRUE))
# Point pour lequel on veut trouver les voisins les plus proches
test_point <- c(0, 0)
# Appliquer la fonction k-NN
result <- k_nn(data, k = 5, x = test_point)
# Afficher les résultats
print(paste("Les indices des k plus proches voisins:", toString(result$nearest_indices)))
print(paste("Le label prédit pour le point de test:", result$predicted_label))
# Fonction KNN
knn <- function(data, k, x) {
# Calcul des distances entre x et toutes les observations du jeu de données
distances <- apply(data[, -ncol(data)], 1, function(row) sqrt(sum((row - x)^2)))
# Création d'un dataframe avec les distances et les labels
neighbors <- data.frame(distance = distances, label = data[, ncol(data)])
# Tri par distance croissante
neighbors <- neighbors[order(neighbors$distance), ]
# Sélection des k plus proches voisins
k_neighbors <- neighbors[1:k, ]
# Calcul du label majoritaire parmi les k voisins
predicted_label <- ifelse(sum(k_neighbors$label == 1) > sum(k_neighbors$label == 0), 1, 0)
return(predicted_label)
}
# Exemple d'utilisation avec une variable aléatoire de loi normale
set.seed(123)  # Pour la reproductibilité
data <- data.frame(X1 = rnorm(100), X2 = rnorm(100), label = sample(0:1, 100, replace = TRUE))
# Choisissez une observation pour laquelle vous voulez prédire le label
observation <- c(0, 0)
# Utilisation de la fonction KNN avec k=5
result <- knn(data, 5, observation)
# Affichage du résultat
print(result)
# Fonction k-plus-proches-voisins avec la norme euclidienne
k_plus_proches_voisins <- function(data, k, x) {
# Calcul des distances euclidiennes entre x et tous les points du jeu de données
distances <- sqrt(rowSums((data[, -ncol(data)] - x)^2))
# Tri des indices des voisins les plus proches
indices_voisins <- order(distances)[1:k]
# Récupération des labels des k voisins les plus proches
labels_voisins <- data[indices_voisins, ncol(data)]
# Retourne les labels des k voisins les plus proches
return(labels_voisins)
}
# Exemple d'utilisation avec une variable aléatoire de loi normale
set.seed(123)
donnees <- data.frame(X1 = rnorm(100), X2 = rnorm(100), Label = sample(0:1, 100, replace = TRUE))
# Point pour lequel on veut trouver les voisins les plus proches
point_test <- c(0, 0)
# Paramètre k
k <- 5
# Utilisation de la fonction
resultat <- k_plus_proches_voisins(donnees, k, point_test)
# Affichage des résultats
cat("Labels des", k, "voisins les plus proches :", resultat, "\n")
# Définition du jeu de données
X= matrix(c(rnorm(100), ncol=2)) # Valeur connue
X1= matrix(c(rnorm(50,0,1),rnorm(50,2,1)),ncol=2, byrow = TRUE) # Valeur de test
Y=c(rep(0,25),rep(1,25)) # Valeur à prédire
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(X1,Y)
plot(X1[,1],X1[,2],col=mycolor[Y+1])
# Fonction k-plus-proches-voisins avec la norme euclidienne
k_plus_proches_voisins <- function(data, k, x) {
# Calcul des distances euclidiennes entre x et tous les points du jeu de données
distances <- sqrt(rowSums((data[, -ncol(data)] - x)^2))
# Tri des indices des voisins les plus proches
indices_voisins <- order(distances)[1:k]
# Récupération des labels des k voisins les plus proches
labels_voisins <- data[indices_voisins, ncol(data)]
# Retourne les labels des k voisins les plus proches
return(labels_voisins)
}
Xnew=matrix(rnorm(100,0,1), rnorm(100,2,1), ncol=2, byrow = TRUE)
Ynew=apply(Xnew,1,k_plus_proches_voisins,Data=Data,k=3)
# Fonction pour calculer la distance Euclidienne entre deux points
euclidean_distance <- function(x1, x2) {
sqrt(sum((x1 - x2)^2))
}
# Fonction pour trouver les k-plus-proches-voisins
knn <- function(data, k, x) {
distances <- apply(data, 1, function(row) euclidean_distance(row[-ncol(data)], x))
nearest_neighbors <- order(distances)[1:k]
return(nearest_neighbors)
}
# Exemple d'utilisation
set.seed(123)  # Pour la reproductibilité des résultats
X <- matrix(c(rnorm(100), ncol = 2))
X1 <- matrix(c(rnorm(50, 0, 1), rnorm(50, 2, 1)), ncol = 2, byrow = TRUE)
Y <- c(rep(0, 25), rep(1, 25))
# Tester avec un point spécifique
point_a_tester <- c(0, 0)  # Remplacez ces valeurs par celles que vous souhaitez tester
k_voisins <- knn(X, k = 5, x = point_a_tester)
# Afficher les k-plus-proches-voisins
cat("Les indices des k-plus-proches-voisins :", k_voisins, "\n")
# Vérifier la condition classique pour le cas binaire
label_pred <- ifelse(mean(Y[k_voisins]) >= 0.5, 1, 0)
cat("Le label prédit pour le point à tester :", label_pred, "\n")
kppv <- function(Data, x, k) {
# Calculer les distances entre x et toutes les observations dans Data
distances <- sqrt(rowSums((Data - x)^2))
# Trier les distances et obtenir les indices des k plus proches voisins
indices_plus_proches <- order(distances)[1:k]
# Extraire les labels correspondants aux k plus proches voisins
labels_plus_proches <- Y[indices_plus_proches]
# Retourner la classe majoritaire parmi les k plus proches voisins
classe_majoritaire <- ifelse(sum(labels_plus_proches == 0) > sum(labels_plus_proches == 1), 0, 1)
return(classe_majoritaire)
}
# Données initiales
set.seed(123) # pour la reproductibilité
X <- matrix(c(rnorm(100), rnorm(100, 2, 1)), ncol = 2)
Y <- c(rep(0, 50), rep(1, 50))
# Nouvelles valeurs
set.seed(456) # pour la reproductibilité
Xnew <- matrix(c(rnorm(100, 0, 1), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
# Appliquer la fonction aux nouvelles valeurs
Ynew <- apply(Xnew, 1, function(x) kppv(Data = X, x = x, k = 3))
# Afficher les résultats
print(Ynew)
# Définition du jeu de données
X= matrix(c(rnorm(100), ncol=2)) # Valeur connue
X1= matrix(c(rnorm(50,0,1),rnorm(50,2,1)),ncol=2, byrow = TRUE) # Valeur de test
Y=c(rep(0,25),rep(1,25)) # Valeur à prédire
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(X1,Y)
plot(X1[,1],X1[,2],col=mycolor[Y+1])
# Définition du jeu de données
X= matrix(c(rnorm(100), ncol=2)) # Valeur connue
X1= matrix(c(rnorm(50,0,1),rnorm(50,2,1)),ncol=2, byrow = TRUE) # Valeur de test
Y=c(rep(0,25),rep(1,25)) # Valeur à prédire
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(X1,Y)
plot(X1[,1],X1[,2],col=mycolor[Y+1])
# Définition du jeu de données
X= matrix(c(rnorm(100), ncol=2)) # Valeur connue
X1= matrix(c(rnorm(50,0,1),rnorm(50,2,1)),ncol=2, byrow = TRUE) # Valeur de test
Y=c(rep(0,25),rep(1,25)) # Valeur à prédire
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(X1,Y)
plot(X1[,1],X1[,2],col=mycolor[Y+1])
# Définition du jeu de données
X= matrix(c(rnorm(100), ncol=2)) # Valeur connue
X1= matrix(c(rnorm(50,0,1),rnorm(50,2,1)),ncol=2, byrow = TRUE) # Valeur de test
Y=c(rep(0,25),rep(1,25)) # Valeur à prédire
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(X1,Y)
plot(X1[,1],X1[,2],col=mycolor[Y+1])
# Définition du jeu de données
X= matrix(c(rnorm(100), ncol=2)) # Valeur connue
X1= matrix(c(rnorm(50,0,1),rnorm(50,2,1)),ncol=2, byrow = TRUE) # Valeur de test
Y=c(rep(0,25),rep(1,25)) # Valeur à prédire
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(X1,Y)
plot(X1[,1],X1[,2],col=mycolor[Y+1])
kppv <- function(Data, x, k) {
# Calculer les distances entre x et toutes les observations dans Data
distances <- sqrt(rowSums((Data - x)^2))
# Trier les distances et obtenir les indices des k plus proches voisins
indices_plus_proches <- order(distances)[1:k]
# Extraire les labels correspondants aux k plus proches voisins
labels_plus_proches <- Y[indices_plus_proches]
# Retourner la classe majoritaire parmi les k plus proches voisins
classe_majoritaire <- ifelse(sum(labels_plus_proches == 0) > sum(labels_plus_proches == 1), 0, 1)
return(classe_majoritaire)
}
# Données initiales
set.seed(123) # pour la reproductibilité
X <- matrix(c(rnorm(100), rnorm(100, 2, 1)), ncol = 2)
Y <- c(rep(0, 50), rep(1, 50))
# Nouvelles valeurs
set.seed(456) # pour la reproductibilité
Xnew <- matrix(c(rnorm(100, 0, 1), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
# Appliquer la fonction aux nouvelles valeurs
Ynew <- apply(Xnew, 1, function(x) kppv(Data = X, x = x, k = 3))
# Afficher les résultats
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(Xnew,Ynew)
plot(Xnew[,1],Xnew[,2],col=mycolor[Ynew+1])
kppv <- function(Data, x, k) {
# Calculer les distances entre x et toutes les observations dans Data
distances <- sqrt(rowSums((Data - x)^2))
# Trier les distances et obtenir les indices des k plus proches voisins
indices_plus_proches <- order(distances)[1:k]
# Extraire les labels correspondants aux k plus proches voisins
labels_plus_proches <- Y[indices_plus_proches]
# Retourner la classe majoritaire parmi les k plus proches voisins
classe_majoritaire <- ifelse(sum(labels_plus_proches == 0) > sum(labels_plus_proches == 1), 0, 1)
return(classe_majoritaire)
}
# Données initiales
set.seed(123) # pour la reproductibilité
X <- matrix(c(rnorm(100), rnorm(100, 2, 1)), ncol = 2)
Y <- c(rep(0, 50), rep(1, 50))
# Nouvelles valeurs
set.seed(456) # pour la reproductibilité
Xnew <- matrix(c(rnorm(100, 0, 1), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
# Appliquer la fonction aux nouvelles valeurs
Ynew <- apply(Xnew, 1, function(x) kppv(Data = X, x = x, k = 3))
# Afficher les résultats
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(Xnew,Ynew)
plot(Xnew[,1],Xnew[,2],col=mycolor[Ynew+1])
kppv <- function(Data, x, k) {
# Calculer les distances entre x et toutes les observations dans Data
distances <- sqrt(rowSums((Data - x)^2))
# Trier les distances et obtenir les indices des k plus proches voisins
indices_plus_proches <- order(distances)[1:k]
# Extraire les labels correspondants aux k plus proches voisins
labels_plus_proches <- Y[indices_plus_proches]
# Retourner la classe majoritaire parmi les k plus proches voisins
classe_majoritaire <- ifelse(sum(labels_plus_proches == 0) > sum(labels_plus_proches == 1), 0, 1)
return(classe_majoritaire)
}
# Données initiales
set.seed(123) # pour la reproductibilité
X <- matrix(c(rnorm(100), rnorm(100, 2, 1)), ncol = 2)
Y <- c(rep(0, 50), rep(1, 50))
# Nouvelles valeurs
set.seed(456) # pour la reproductibilité
Xnew <- matrix(c(rnorm(100, 0, 1), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
# Appliquer la fonction aux nouvelles valeurs
Ynew <- apply(Xnew, 1, function(x) kppv(Data = X, x = x, k = 3))
# Afficher les résultats
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(Xnew,Ynew)
plot(Xnew[,1],Xnew[,2],col=mycolor[Ynew+1])
kppv <- function(Data, x, k) {
# Calculer les distances entre x et toutes les observations dans Data
distances <- sqrt(rowSums((Data - x)^2))
# Trier les distances et obtenir les indices des k plus proches voisins
indices_plus_proches <- order(distances)[1:k]
# Extraire les labels correspondants aux k plus proches voisins
labels_plus_proches <- Y[indices_plus_proches]
# Retourner la classe majoritaire parmi les k plus proches voisins
classe_majoritaire <- ifelse(sum(labels_plus_proches == 0) > sum(labels_plus_proches == 1), 0, 1)
return(classe_majoritaire)
}
# Données initiales
set.seed(123) # pour la reproductibilité
X <- matrix(c(rnorm(100), rnorm(100, 2, 1)), ncol = 2)
Y <- c(rep(0, 50), rep(1, 50))
# Nouvelles valeurs
set.seed(456) # pour la reproductibilité
Xnew <- matrix(c(rnorm(100, 0, 1), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
# Appliquer la fonction aux nouvelles valeurs
Ynew <- apply(Xnew, 1, function(x) kppv(Data = X, x = x, k = 3))
# Afficher les résultats
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(Xnew,Ynew)
plot(Xnew[,1],Xnew[,2],col=mycolor[Ynew+1])
kppv <- function(Data, x, k) {
# Calculer les distances entre x et toutes les observations dans Data
distances <- sqrt(rowSums((Data - x)^2))
# Trier les distances et obtenir les indices des k plus proches voisins
indices_plus_proches <- order(distances)[1:k]
# Extraire les labels correspondants aux k plus proches voisins
labels_plus_proches <- Y[indices_plus_proches]
# Retourner la classe majoritaire parmi les k plus proches voisins
classe_majoritaire <- ifelse(sum(labels_plus_proches == 0) > sum(labels_plus_proches == 1), 0, 1)
return(classe_majoritaire)
}
# Données initiales
set.seed(123) # pour la reproductibilité
X <- matrix(c(rnorm(100), rnorm(100, 2, 1)), ncol = 2)
Y <- c(rep(0, 50), rep(1, 50))
# Nouvelles valeurs
set.seed(456) # pour la reproductibilité
Xnew <- matrix(c(rnorm(100, 0, 1), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
# Appliquer la fonction aux nouvelles valeurs
Ynew <- apply(Xnew, 1, function(x) kppv(Data = X, x = x, k = 3))
# Afficher les résultats
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(Xnew,Ynew)
plot(Xnew[,1],Xnew[,2],col=mycolor[Ynew+1])
kppv <- function(Data, x, k) {
# Calculer les distances entre x et toutes les observations dans Data
distances <- sqrt(rowSums((Data - x)^2))
# Trier les distances et obtenir les indices des k plus proches voisins
indices_plus_proches <- order(distances)[1:k]
# Extraire les labels correspondants aux k plus proches voisins
labels_plus_proches <- Y[indices_plus_proches]
# Retourner la classe majoritaire parmi les k plus proches voisins
classe_majoritaire <- ifelse(sum(labels_plus_proches == 0) > sum(labels_plus_proches == 1), 0, 1)
return(classe_majoritaire)
}
# Données initiales
#set.seed(123) # pour la reproductibilité
X <- matrix(c(rnorm(100), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
Y <- c(rep(0, 50), rep(1, 50))
# Nouvelles valeurs
set.seed(456) # pour la reproductibilité
Xnew <- matrix(c(rnorm(100, 0, 1), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
# Appliquer la fonction aux nouvelles valeurs
Ynew <- apply(Xnew, 1, function(x) kppv(Data = X, x = x, k = 3))
# Afficher les résultats
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(Xnew,Ynew)
plot(Xnew[,1],Xnew[,2],col=mycolor[Ynew+1])
kppv <- function(Data, x, k) {
# Calculer les distances entre x et toutes les observations dans Data
distances <- sqrt(rowSums((Data - x)^2))
# Trier les distances et obtenir les indices des k plus proches voisins
indices_plus_proches <- order(distances)[1:k]
# Extraire les labels correspondants aux k plus proches voisins
labels_plus_proches <- Y[indices_plus_proches]
# Retourner la classe majoritaire parmi les k plus proches voisins
classe_majoritaire <- ifelse(sum(labels_plus_proches == 0) > sum(labels_plus_proches == 1), 0, 1)
return(classe_majoritaire)
}
# Données initiales
#set.seed(123) # pour la reproductibilité
X <- matrix(c(rnorm(100), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
Y <- c(rep(0, 50), rep(1, 50))
# Nouvelles valeurs
set.seed(456) # pour la reproductibilité
Xnew <- matrix(c(rnorm(100, 0, 1), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
# Appliquer la fonction aux nouvelles valeurs
Ynew <- apply(Xnew, 1, function(x) kppv(Data = X, x = x, k = 3))
# Afficher les résultats
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(Xnew,Ynew)
plot(Xnew[,1],Xnew[,2],col=mycolor[Ynew+1])
kppv <- function(Data, x, k) {
# Calculer les distances entre x et toutes les observations dans Data
distances <- sqrt(rowSums((Data - x)^2))
# Trier les distances et obtenir les indices des k plus proches voisins
indices_plus_proches <- order(distances)[1:k]
# Extraire les labels correspondants aux k plus proches voisins
labels_plus_proches <- Y[indices_plus_proches]
# Retourner la classe majoritaire parmi les k plus proches voisins
classe_majoritaire <- ifelse(sum(labels_plus_proches == 0) > sum(labels_plus_proches == 1), 0, 1)
return(classe_majoritaire)
}
# Données initiales
#set.seed(123) # pour la reproductibilité
X <- matrix(c(rnorm(100), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
Y <- c(rep(0, 50), rep(1, 50))
# Nouvelles valeurs
#set.seed(456) # pour la reproductibilité
Xnew <- matrix(c(rnorm(100, 0, 1), rnorm(100, 2, 1)), ncol = 2, byrow = TRUE)
# Appliquer la fonction aux nouvelles valeurs
Ynew <- apply(Xnew, 1, function(x) kppv(Data = X, x = x, k = 3))
# Afficher les résultats
mycolor=c("black","red") # définir le vecteur couleur
Data=cbind(Xnew,Ynew)
plot(Xnew[,1],Xnew[,2],col=mycolor[Ynew+1])
setwd("C:/Users/HP/OneDrive - UNIVERSITE DES LAGUNES/COURS_DE_KRA/3EME_ANNEE/Semestre2/SCIENCE/STAT_APP")
