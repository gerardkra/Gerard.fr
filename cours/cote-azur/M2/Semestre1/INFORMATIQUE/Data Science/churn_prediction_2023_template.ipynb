{
  "metadata": {
    "name": "churn_prediction_2023_template",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.tuning.ParamGridBuilder\nimport org.apache.spark.ml.tuning.CrossValidator\nimport org.apache.spark.ml.feature.VectorAssembler"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\nCustomer Relationship Management (CRM) is a key element of modern marketing strategies.\nThe French Telecom company Orange offered a large marketing database to predict the propensity of customers to switch provider (churn).\n\nOrange made available a large dataset of customer data consisting of:\n- Training : 50,000 instances including 15,000 inputs variables, and the target value.\n- Test : 50,000 instances including 15,000 inputs variables.\n\nSee details on https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Intro\n\nThe Orange Telecom Churn Dataset used in this labs consists of cleaned customer activity data (features), along with a churn label specifying whether the customer canceled their subscription or not. \nThe used dataset is smaller than the original one.\nThe input csv file has the following format: \nKS,128,415,No,Yes,25,265.1,110,45.07,197.4,99,16.78,244.7,91,11.01,10.0,3,2.7,1,False\nThe name of the variables are given in the header of the data files.\n\nWe use a Scala case class and Structype to define the schema, corresponding to a line in the csv data file."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\ncase class Account(state: String, len: Integer, acode: String,\n    intlplan: String, vplan: String, numvmail: Double,\n    tdmins: Double, tdcalls: Double, tdcharge: Double,\n    temins: Double, tecalls: Double, techarge: Double,\n    tnmins: Double, tncalls: Double, tncharge: Double,\n    timins: Double, ticalls: Double, ticharge: Double,\n    numcs: Double, churn: String)\n    \n    \n// StructType objects define the schema of Spark DataFrames. \n// StructType objects contain a list of StructField objects that define the name, type, and nullable flag for each column in a DataFrame.    \n// A nullable flag switched to true means that the corresponding column could have null values.\nval schema \u003d StructType(Array(\n    StructField(\"state\", StringType, true),\n    StructField(\"len\", IntegerType, true),\n    StructField(\"acode\", StringType, true),\n    StructField(\"intlplan\", StringType, true),\n    StructField(\"vplan\", StringType, true),\n    StructField(\"numvmail\", DoubleType, true),\n    StructField(\"tdmins\", DoubleType, true),\n    StructField(\"tdcalls\", DoubleType, true),\n    StructField(\"tdcharge\", DoubleType, true),\n    StructField(\"temins\", DoubleType, true),\n    StructField(\"tecalls\", DoubleType, true),\n    StructField(\"techarge\", DoubleType, true),\n    StructField(\"tnmins\", DoubleType, true),\n    StructField(\"tncalls\", DoubleType, true),\n    StructField(\"tncharge\", DoubleType, true),\n    StructField(\"timins\", DoubleType, true),\n    StructField(\"ticalls\", DoubleType, true),\n    StructField(\"ticharge\", DoubleType, true),\n    StructField(\"numcs\", DoubleType, true),\n    StructField(\"churn\", StringType, true)\n  ))"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval trainDf \u003d spark.read.option(\"header\", \"true\").csv(\"/lab7/churn-train-header.csv\")\ntrainDf.count()\ntrainDf.first()\ntrainDf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nimport spark.implicits._\nval train: Dataset[Account] \u003d spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\")\n      .schema(schema).csv(\"/lab7/churn-train-header.csv\").as[Account]\ntrain.count()  \ntrain.cache()\ntrain.first()\ntrain.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Q1: What is the advantage to use the \"Dataset\" train instead of the \"Dataframe\" trainDf?**\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval test: Dataset[Account] \u003d spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\")\n      .schema(schema).csv(\"/lab7/churn-test-header.csv\").as[Account]\ntest.count()      \ntest.first()\ntest.printSchema()\n    "
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\ntrain.createOrReplaceTempView(\"account\")\nspark.catalog.cacheTable(\"account\") // Caches the specified table in-memory.\ntrain.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The describe() function performs summary statistics calculations on  numeric columns "
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\ntrain.describe(\"tdcharge\", \"techarge\",\"tncharge\", \"numcs\").show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Q2: Compute the pearson correlation between tdmins and tdcharge in the following cell**"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nimport org.apache.spark.mllib.stat.Statistics\n\nval tdmins \u003d train.select(\"tdmins\").map{row:Row \u003d\u003e row.getAs[Double](\"tdmins\")}.rdd\nval tdcharge \u003d train.select( \"tdcharge\").map{row:Row \u003d\u003e row.getAs[Double](\"tdcharge\")}.rdd\n\nval correlation \u003d ..."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \nuse Spark SQL to explore the dataset"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Q3: compute the number of labels (churn or no churn) for the train dataset in the following cell. Use sparkSQL.**"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// Compute the number of labels hereafter\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval fractions \u003d Map(\"False\" -\u003e .17, \"True\" -\u003e 1.0)\n//Here we\u0027re keeping all instances of the Churn\u003dTrue class, but downsampling the Churn\u003dFalse class to a fraction of 388/2278.\nval strain \u003d train.stat.sampleBy(\"churn\", fractions, 36L) // Returns a stratified sample without replacement based on the fraction given on each stratum.\n// Parameters:\n//    col - column that defines strata\n//    fractions - sampling fraction for each stratum. If a stratum is not specified, we treat its fraction as zero.\n//    seed - random seed\n\nstrain.groupBy(\"churn\").count.show\nstrain.createOrReplaceTempView(\"account\")\nspark.catalog.cacheTable(\"account\")\n\n// The resulting table is now balanced (\"fractions\" defines a probability to take a sample, hence the number of samples is not known in advance)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The zeppelin-context is a system-wide container for common utility functions and user-specific data. \nIt implements functions for data input, data display, etc. that are often needed but are not uniformly available in all interpreters.\n\nThe zeppelin-context is available as a predefined variable z that can be used by directly invoking its methods. \n\nIn the Apache Spark interpreter, the zeppelin-context provides a show method, which, using Zeppelin\u0027s table feature, can be used to nicely display a set of data."
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark2.spark\n\nz.show(strain.groupBy(\"churn\").avg())"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT churn, avg(numcs) as numcs\nFROM account \nGROUP BY churn\nORDER BY numcs"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Q4: Use a query \"SELECT ...\" to show the average of tdmins, temins and tnmins for each label (churn \u003d true or churn \u003d false) in the following cell.**"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\n-- Put your query here\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval ntrain \u003d strain.drop(\"state\").drop(\"acode\").drop(\"vplan\").drop(\"tdcharge\").drop(\"techarge\").drop(\"ticharge\")\nprintln(ntrain.count)\nntrain.show\nntrain.cache"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In order for the features to be used by a machine learning algorithm, they are transformed into numbers representing the value for each feature\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval ipindexer \u003d new StringIndexer()\n      .setInputCol(\"intlplan\")\n      .setOutputCol(\"iplanIndex\")\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Q5: Create a transformer to convert the churn value into a 0-1 label**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// Put the transformer here\n\nval labelindexer \u003d ...\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Q6: Create a transformer to convert all the features in a single vector**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// Put the transformer here\n\nval featureCols \u003d Array(...)\nval assembler \u003d ...\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Decision trees have played a significant role in data mining and machine learning since the 1960\u0027s. \nThey generate white-box classification and regression models which can be used for feature selection and sample prediction. \nThe transparency of these models is a big advantage over black-box learners, because the models are easy to understand and interpret, and they can be readily extracted and implemented in any programming language (with nested if-else statements) for use in production environments. \nFurthermore, decision trees require almost no data preparation (ie normalization) and can handle both categorical and continuous data. \nTo remedy over-fitting and improve prediction accuracy, decision trees can also be limited to a certain depth or complexity, or bundled into ensembles of trees (ie random forests).\n\nA decision tree is a predictive model which maps observations (features) about an item to conclusions about the item\u0027s label or class. \nThe model is generated using a top-down approach, where the source dataset is split into subsets using a statistical measure, often in the form of the Gini index or information gain via Shannon entropy. \nThis process is applied recursively until a subset contains only samples with the same target class, or is halted by a predefined stopping criteria."
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval dTree \u003d new DecisionTreeClassifier().setLabelCol(\"label\")\n      .setFeaturesCol(\"features\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Set up a pipeline to pass the data through transformers to extract the features and label and pass this to a decision tree estimator to fit the model \n\n**Q7: Create the Pipeline to train the decision tree**\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// put treeClassifier in a Pipeline here.\n val pipeline \u003d ...\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\nSpark ML supports k-fold cross validation with a transformation/estimation pipeline to try out different combinations of parameters, using a process called grid search. \r\nYou set up a CrossValidator with the parameters to test, an estimator and evaluator for a model selection workflow.\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// set param grid to Search through decision tree\u0027s maxDepth parameter for best model\n// Deeper trees are potentially more accurate, but are also more likely to overfit.\n val paramGrid \u003d new ParamGridBuilder().addGrid(dTree.maxDepth, Array( 4, 5, 6)).build()\n val evaluator \u003d new BinaryClassificationEvaluator()\n      .setLabelCol(\"label\")\n      .setRawPredictionCol(\"prediction\")\n\n// Set up 3-fold cross validation with paramGrid\n val crossval \u003d new CrossValidator().setCollectSubModels(true).setEstimator(pipeline)\n      .setEvaluator(evaluator)\n      .setEstimatorParamMaps(paramGrid).setNumFolds(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval cvModel \u003d crossval.fit(ntrain) // Model from k-fold cross validation"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\ncvModel.subModels.foreach{println}"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval bestModel \u003d cvModel.bestModel // retrieve only the best model\nprintln(\"The Best Model:\\n--------------------\")\n// Use Scala\u0027s asInstanceOf method to cast an instance to the desired type.\nval treeModel \u003d bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel].stages(3).asInstanceOf[DecisionTreeClassificationModel] // extract the decision tree model from the pipeline\nprintln(\"Learned classification tree model:\\n\" + treeModel.toDebugString) // print the decision tree: we can read the sequence of decision rules"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Q8: What are the two first features used in the decision tree based classification rule? Give their name with respect to the original dataset.**\n"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\r\r// Put here the command to print the original names of the features.\r\rprintln(...) // print the name of the columns which appear first in the decision tree\r"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\nThe actual performance of the model can be determined using the test data set which has not been used for any training or cross-validation activities. \r\nWe\u0027ll transform the test set with the model pipeline, which will map the features according to the same recipe. \r\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Q9: Appy the learned transformer on the test dataset.**"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// Put the command to compute the \"predictions\" hereafter.\n// The variable \"predictions\" will contrain the result of the transformer.\n\n//transform the test set with the model pipeline, which will map the features according to the same recipe\nval predictions \u003d ...\n\ntest.printSchema()\n\npredictions.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\npredictions.select(\"label\",\"prediction\", \"probability\").show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Accuracy is measured by the area under the ROC curve. The area measures the ability of the test to correctly classify true positives from false positives. A random predictor would have .5 accuracy. The closer the value is to 1 the better its predictions are. \n\n**Q10: Compute the area under the ROC curve in the following cell.**"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// Put your command line to compute the variable \"accuracy\" hereafter\n\n//The evaluator will provide us with the score of the predictions by comparing the prediction to the label\nval accuracy \u003d ...\n\nevaluator.explainParams()"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval lp \u003d predictions.select(\"label\", \"prediction\")\nval counttotal \u003d predictions.count()\nval correct \u003d lp.filter($\"label\" \u003d\u003d\u003d $\"prediction\").count() // comparison between columns\nval wrong \u003d lp.filter(not($\"label\" \u003d\u003d\u003d $\"prediction\")).count()\nval ratioWrong \u003d wrong.toDouble / counttotal.toDouble\nval ratioCorrect \u003d correct.toDouble / counttotal.toDouble\nval truep \u003d lp.filter($\"prediction\" \u003d\u003d\u003d 0.0).filter($\"label\" \u003d\u003d\u003d $\"prediction\").count() / counttotal.toDouble\nval truen \u003d lp.filter($\"prediction\" \u003d\u003d\u003d 1.0).filter($\"label\" \u003d\u003d\u003d $\"prediction\").count() / counttotal.toDouble\nval falsep \u003d lp.filter($\"prediction\" \u003d\u003d\u003d 1.0).filter(not($\"label\" \u003d\u003d\u003d $\"prediction\")).count() / counttotal.toDouble\nval falsen \u003d lp.filter($\"prediction\" \u003d\u003d\u003d 0.0).filter(not($\"label\" \u003d\u003d\u003d $\"prediction\")).count() / counttotal.toDouble\nprintln(\"total count : \" + counttotal)\nprintln(\"correct : \" + correct)\nprintln(\"wrong: \" + wrong)\nprintln(\"ratio correct: \" + ratioCorrect)\nprintln(\"ratio true positive : \" + truep)\nprintln(\"ratio true negative : \" + truen)\nprintln(\"ratio wrong: \" + ratioWrong)\nprintln(\"ratio false positive : \" + falsep)\nprintln(\"ratio false negative : \" + falsen)\n"
    }
  ]
}