{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83118dab",
   "metadata": {},
   "source": [
    "\n",
    "# Algorithmes d'Optimisation dans le Cas Convexe\n",
    "\n",
    "Ce notebook explore différentes méthodes d'optimisation appliquées à un problème de régression linéaire.\n",
    "Nous allons implémenter et comparer plusieurs algorithmes d'optimisation gradient-based :\n",
    "\n",
    "- **Gradient Descent (GD)**\n",
    "- **Stochastic Gradient Descent (SGD)**\n",
    "- **SAGA (Stochastic Average Gradient Algorithm)**\n",
    "- **SVRG (Stochastic Variance Reduced Gradient)**\n",
    "\n",
    "Nous considérons le problème d'optimisation suivant :\n",
    "\n",
    "\\[ \\min_{w} \\frac{1}{2} || A w - b ||^2 \\]\n",
    "\n",
    "où :\n",
    "\n",
    "- \\( A \\in \\mathbb{R}^{n \\times d} \\) est la matrice des caractéristiques.\n",
    "- \\( b \\in \\mathbb{R}^{n} \\) est le vecteur des valeurs cibles.\n",
    "- \\( w \\in \\mathbb{R}^{d} \\) représente les paramètres à optimiser.\n",
    "\n",
    "Chaque cellule de code est précédée d'une explication et des formules mathématiques associées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0463c84",
   "metadata": {},
   "source": [
    "## 1️⃣ Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fbd11",
   "metadata": {},
   "source": [
    "\n",
    "## 2️⃣ Initialisation des paramètres\n",
    "\n",
    "Nous définissons une fonction pour initialiser les paramètres d'un modèle avec une valeur spécifique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae6fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_para(nn, val):\n",
    "    for para in nn.parameters():\n",
    "        para.data.fill_(val)\n",
    "    return nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8691d99",
   "metadata": {},
   "source": [
    "\n",
    "## 3️⃣ Définition du modèle de régression linéaire\n",
    "\n",
    "Nous implémentons un modèle de régression linéaire basé sur PyTorch. Ce modèle suit la fonction :\n",
    "\n",
    "\\[ f(w) = A w \\]\n",
    "\n",
    "Nous définissons également plusieurs méthodes d'optimisation :\n",
    "\n",
    "- **Calcul du gradient complet**.\n",
    "- **Gradient partiel pour les méthodes stochastiques**.\n",
    "- **Méthodes d'optimisation GD, SGD, SAGA, et SVRG**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_samples, n_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "        self.A = Variable(torch.randn(n_samples, n_features), requires_grad=False)\n",
    "        self.b = Variable(torch.randn(n_samples, 1), requires_grad=False)\n",
    "        self.w = nn.Parameter(torch.zeros(n_features, 1), requires_grad=True)\n",
    "\n",
    "    def forward(self):\n",
    "        return torch.mm(self.A, self.w)\n",
    "\n",
    "    def full_loss_grad(self, criterion):\n",
    "        grad = 0\n",
    "        output = self.forward()\n",
    "        loss = criterion(output, self.b)\n",
    "        loss.backward()\n",
    "        for para in self.parameters():\n",
    "            grad += para.grad.data.norm(2) ** 2\n",
    "        return loss.data.item(), (1.0 / self.n_samples) * np.sqrt(grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5b839",
   "metadata": {},
   "source": [
    "\n",
    "## 4️⃣ Descente de Gradient (GD)\n",
    "\n",
    "L'algorithme **Gradient Descent** suit la mise à jour :\n",
    "\n",
    "\\[ w^{(t+1)} = w^{(t)} - \\eta \\nabla L(w) \\]\n",
    "\n",
    "où \\( \\eta \\) est le taux d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gd_backward(self, criterion, max_iter, lr):\n",
    "    total_loss_epoch = [0 for _ in range(max_iter)]\n",
    "    grad_norm_epoch = [0 for _ in range(max_iter)]\n",
    "    for epoch in range(max_iter):\n",
    "        self.zero_grad()\n",
    "        total_loss_epoch[epoch], grad_norm_epoch[epoch] = self.full_loss_grad(criterion)\n",
    "        for i_data in range(self.n_samples):\n",
    "            self.zero_grad()\n",
    "            _ = self.grad(criterion)\n",
    "            for para in self.parameters():\n",
    "                para.data -= lr * para.grad.data\n",
    "    return total_loss_epoch, grad_norm_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22afadf7",
   "metadata": {},
   "source": [
    "\n",
    "## 5️⃣ Descente de Gradient Stochastique (SGD)\n",
    "\n",
    "L'algorithme **SGD** met à jour les poids après chaque échantillon aléatoire :\n",
    "\n",
    "\\[ w^{(t+1)} = w^{(t)} - \\eta \\nabla L_i(w) \\]\n",
    "\n",
    "où \\( \\nabla L_i(w) \\) est le gradient basé sur un seul échantillon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sgd_backward(self, criterion, max_iter, lr):\n",
    "    total_loss_epoch = [0 for _ in range(max_iter)]\n",
    "    grad_norm_epoch = [0 for _ in range(max_iter)]\n",
    "    for epoch in range(max_iter):\n",
    "        self.zero_grad()\n",
    "        total_loss_epoch[epoch], grad_norm_epoch[epoch] = self.full_loss_grad(criterion)\n",
    "        for i_data in range(self.n_samples):\n",
    "            self.zero_grad()\n",
    "            _, loss = self.partial_grad(criterion)\n",
    "            for para in self.parameters():\n",
    "                para.data -= lr * para.grad.data\n",
    "    return total_loss_epoch, grad_norm_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e20e9c",
   "metadata": {},
   "source": [
    "\n",
    "## 6️⃣ Méthode SAGA\n",
    "\n",
    "L'algorithme **SAGA** corrige la variance du gradient en utilisant une mémoire des gradients passés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saga_backward(self, criterion, n_epoch, learning_rate):\n",
    "    grad_norm_epoch = [0 for _ in range(n_epoch)]\n",
    "    total_loss_epoch = [0 for _ in range(n_epoch)]\n",
    "    mean_grad = copy.deepcopy(self)\n",
    "    prev_stoc_grad = [copy.deepcopy(self) for _ in range(self.n_samples)]\n",
    "    epoch = 0\n",
    "    for i in range(n_epoch * self.n_samples):\n",
    "        if i % self.n_samples == 0:\n",
    "            self.zero_grad()\n",
    "            total_loss_epoch[epoch], grad_norm_epoch[epoch] = self.full_loss_grad(criterion)\n",
    "            epoch += 1\n",
    "        self.zero_grad()\n",
    "        i_data, loss = self.partial_grad(criterion)\n",
    "        for para, para_prev, para_mean in zip(self.parameters(), prev_stoc_grad[i_data].parameters(), mean_grad.parameters()):\n",
    "            saga_update = para.grad.data - para_prev.data + para_mean.data\n",
    "            para_mean.data += (1.0 / self.n_samples) * (para.grad.data - para_prev.data)\n",
    "            para_prev.data = para.grad.data.clone()\n",
    "            para.data.sub_(saga_update * learning_rate)\n",
    "    return total_loss_epoch, grad_norm_epoch\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
