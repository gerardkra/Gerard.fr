{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38b862c",
   "metadata": {},
   "source": [
    "\n",
    "# Algorithmes d'Optimisation dans le Cas Convexe\n",
    "\n",
    "Ce notebook explore différentes méthodes d'optimisation appliquées à un problème de régression linéaire.\n",
    "Nous allons implémenter et comparer plusieurs algorithmes :\n",
    "\n",
    "- **Gradient Descent (GD)**\n",
    "- **Stochastic Gradient Descent (SGD)**\n",
    "- **SAGA**\n",
    "- **SVRG (Stochastic Variance Reduced Gradient)**\n",
    "\n",
    "Chaque cellule de code est précédée d'une explication et des formules mathématiques associées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05328f",
   "metadata": {},
   "source": [
    "## Objectif : Importer les bibliothèques nécessaires.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c6ed0",
   "metadata": {},
   "source": [
    "## Objectif : Définir une fonction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aba4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_para(nn, val):\n",
    "\tfor para in nn.parameters():\n",
    "\t\tpara.data.fill_(val)\n",
    "\treturn nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9bacae",
   "metadata": {},
   "source": [
    "## Objectif : Définir une fonction.\n",
    "\n",
    "**Formules associées :**\n",
    "\n",
    "- \\[n_samples\\]\n",
    "- \\[n_features\\]\n",
    "- \\[A\\]\n",
    "- \\[requires_grad\\]\n",
    "- \\[b\\]\n",
    "\n",
    "**Commentaires du code :**\n",
    "\n",
    "- ##########GD Methodes\n",
    "- #Compute full grad\n",
    "- # grad = 0\n",
    "- ###########SGD Methodes\n",
    "- #Compute full grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    '''Docstring fot the min{x}||A*x - b||**2 problem'''\n",
    "    def __init__(self, n_samples, n_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "        self.A = Variable(torch.randn(n_samples, n_features), requires_grad = False)\n",
    "        self.b = Variable(torch.randn(n_samples, 1), requires_grad = False)\n",
    "        self.w = nn.Parameter(torch.zeros(n_features, 1), requires_grad=True)\n",
    "    \n",
    "    def forward(self):\n",
    "        '''Compute the function A*x'''\n",
    "        return torch.mm(self.A, self.w)\n",
    "    \n",
    "    def full_loss_grad(self, criterion):\n",
    "        '''Compute the gradient norm and the full loss'''\n",
    "        grad = 0\n",
    "        output = self.forward()\n",
    "        loss = criterion(output, self.b)\n",
    "        loss.backward()\n",
    "        for para in self.parameters():\n",
    "            grad += para.grad.data.norm(2)**2\n",
    "        return loss.data[0], (1./self.n_samples) * np.sqrt(grad)\n",
    "\n",
    "\n",
    "    def partial_grad(self, criterion):\n",
    "        '''Compute partial gradient of f'''\n",
    "        i = np.random.randint(0, len(self.b))\n",
    "        output = self.forward()\n",
    "        loss = criterion(output[i], self.b[i])\n",
    "        loss.backward()\n",
    "        return i, loss\n",
    "    \n",
    "    def grad(self, criterion):\n",
    "        '''compute the gradient'''\n",
    "        output = self.forward()\n",
    "        loss = criterion(output, self.b)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    ##########GD Methodes\n",
    "\n",
    "    def gd_backward(self, criterion, max_iter, lr):\n",
    "        '''\n",
    "        Compute the gd algorithm\n",
    "        inputs : neural net, loss_function, number of epochs, learning rate\n",
    "        goal : get a minimum\n",
    "        return : total_loss_epoch, grad_norm_epoch\n",
    "        '''\n",
    "        total_loss_epoch = [0 for i in range(n_epoch)]\n",
    "        grad_norm_epoch = [0 for i in range(n_epoch)]\n",
    "        for epoch in range(n_epoch):\n",
    "            #Compute full grad\n",
    "            self.zero_grad() # grad = 0\n",
    "            total_loss_epoch[epoch], grad_norm_epoch[epoch] = self.full_loss_grad(criterion)\n",
    "            for i_data in range(self.n_samples):\n",
    "                self.zero_grad()\n",
    "                _ = self.grad(criterion)\n",
    "                for para in self.parameters():\n",
    "                    para.data -= lr * para.grad.data\n",
    "\n",
    "        return total_loss_epoch, grad_norm_epoch\n",
    "\n",
    "\n",
    "    ###########SGD Methodes\n",
    "\n",
    "    \n",
    "    def sgd_backward(self, criterion, max_iter, lr):\n",
    "        '''\n",
    "        Compute the sgd algorithm\n",
    "        inputs : neural net, loss_function, number of epochs, learning rate\n",
    "        goal : get a minimum\n",
    "        return : total_loss_epoch, grad_norm_epoch\n",
    "        '''\n",
    "        total_loss_epoch = [0 for i in range(n_epoch)]\n",
    "        grad_norm_epoch = [0 for i in range(n_epoch)]\n",
    "        for epoch in range(n_epoch):\n",
    "            #Compute full grad\n",
    "            self.zero_grad() # grad = 0\n",
    "            total_loss_epoch[epoch], grad_norm_epoch[epoch] = self.full_loss_grad(criterion)\n",
    "            for i_data in range(self.n_samples):\n",
    "                self.zero_grad()\n",
    "                _, loss = self.partial_grad(criterion)\n",
    "                for para in self.parameters():\n",
    "                    para.data -= lr * para.grad.data\n",
    "\n",
    "        return total_loss_epoch, grad_norm_epoch\n",
    "\n",
    "    \n",
    "########### SAGA Methodes\n",
    "\n",
    "    def saga_backward(self, criterion, n_epoch, learning_rate):\n",
    "        '''\n",
    "        Compute the saga algorithm\n",
    "        inputs : neural net, loss_function, number of epochs, learning rate\n",
    "        goal : get a minimum\n",
    "        return : total_loss_epoch, grad_norm_epoch\n",
    "        '''\n",
    "        grad_norm_epoch = [0 for i in range(n_epoch)]\n",
    "        total_loss_epoch = [0 for i in range(n_epoch)]\n",
    "        mean_grad = copy.deepcopy(self)#Simple net\n",
    "        prev_stoc_grad = [copy.deepcopy(self) for i in range(self.n_samples)]#list of nets (one for each sample)\n",
    "        epoch = 0\n",
    "        for i in range(n_epoch * self.n_samples):\n",
    "            if i % self.n_samples == 0:\n",
    "                #Calculate loss & grad\n",
    "                self.zero_grad()\n",
    "                total_loss_epoch[epoch], grad_norm_epoch[epoch] = self.full_loss_grad(criterion)\n",
    "                epoch += 1\n",
    "\n",
    "            # Cur stoc grad\n",
    "            self.zero_grad()\n",
    "            i_data, loss = self.partial_grad(criterion)\n",
    "\n",
    "            # backprop\n",
    "            for para, para_prev, para_mean in zip(self.parameters(), prev_stoc_grad[i_data].parameters(), mean_grad.parameters()):\n",
    "                saga_update = para.grad.data - para_prev.data + para_mean.data\n",
    "                para_mean.data += (1./self.n_samples) * (para.grad.data - para_prev.data) #update mean\n",
    "                para_prev.data = para.grad.data.clone() # update previous grad\n",
    "                para.data.sub_(saga_update * learning_rate)\n",
    "\n",
    "        return total_loss_epoch, grad_norm_epoch\n",
    "\n",
    "    \n",
    "########### SVRG Methodes\n",
    "\n",
    "    def partial_grad_svrg(self, self_prev, criterion):\n",
    "        '''Compute partial gradient of f for SVRG backward'''\n",
    "        i = np.random.randint(0, len(self.b))\n",
    "        self.zero_grad() #grad = 0\n",
    "        self_prev.zero_grad() #grad = 0\n",
    "        \n",
    "        #cur_grad\n",
    "        output = self.forward()\n",
    "        loss = criterion(output[i], self.b[i])\n",
    "        loss.backward()\n",
    "        \n",
    "        #prev_grad\n",
    "        output2 = self_prev.forward()\n",
    "        loss2 = criterion(output2[i], self.b[i])\n",
    "        loss2.backward()\n",
    "        \n",
    "        return i, loss, loss2\n",
    "    \n",
    "    def svrg_backward(self, criterion, n_epoch, learning_rate):\n",
    "        \"\"\"\n",
    "        Function to update weights with a SVRG backpropagation\n",
    "        args : loss function, number of epochs, learning rate\n",
    "        return : total_loss_epoch, grad_norm_epoch\n",
    "        \"\"\"\n",
    "        total_loss_epoch = [0 for i in range(n_epoch)]\n",
    "        grad_norm_epoch = [0 for i in range(n_epoch)]\n",
    "        for epoch in range(n_epoch):\n",
    "            running_loss = 0.0\n",
    "            previous_net_sgd = copy.deepcopy(self) #update previous_net_sgd\n",
    "            previous_net_grad = copy.deepcopy(self) #update previous_net_grad\n",
    "            \n",
    "            #Compute full grad\n",
    "            previous_net_grad.zero_grad() # grad = 0\n",
    "            total_loss_epoch[epoch], grad_norm_epoch[epoch] = previous_net_grad.full_loss_grad(criterion)\n",
    "\n",
    "            #Run over the dataset\n",
    "            for i_data in range(self.n_samples):\n",
    "                #Compute cur stoc grad and prev stoc grad\n",
    "                _, cur_loss, prev_loss = self.partial_grad_svrg(previous_net_sgd, criterion)\n",
    "                \n",
    "                #Backward\n",
    "                for param1, param2, param3 in zip(self.parameters(), previous_net_sgd.parameters(), previous_net_grad.parameters()): \n",
    "                    param1.data -= (learning_rate) * (param1.grad.data - param2.grad.data + (1./self.n_samples) * param3.grad.data)\n",
    "                \n",
    "        return total_loss_epoch, grad_norm_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42efbfcb",
   "metadata": {},
   "source": [
    "## Objectif : Effectuer des calculs ou des opérations d'optimisation.\n",
    "\n",
    "**Formules associées :**\n",
    "\n",
    "- \\[loss_function\\]\n",
    "- \\[n_features\\]\n",
    "- \\[net_svrg\\]\n",
    "- \\[net_sgd\\]\n",
    "- \\[net_saga\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "n_samples, n_features = 30, 5\n",
    "net_svrg = Model(n_samples, n_features)\n",
    "net_sgd = copy.deepcopy(net_svrg)\n",
    "net_saga = copy.deepcopy(net_svrg)\n",
    "net_gd = copy.deepcopy(net_svrg)\n",
    "\n",
    "n_epoch = 3000\n",
    "L = 1./(torch.mm(torch.t(net_sgd.A), net_sgd.A).data.norm(2))\n",
    "lr_sgd = L\n",
    "lr_svrg = 1 / (5 * torch.max(torch.mm(torch.t(net_svrg.A), net_svrg.A).sum(1)).data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d34683e",
   "metadata": {},
   "source": [
    "## Objectif : Effectuer des calculs ou des opérations d'optimisation.\n",
    "\n",
    "**Formules associées :**\n",
    "\n",
    "- \\[grad_norm_epoch_gd\\]\n",
    "- \\[grad_norm_epoch_sgd\\]\n",
    "- \\[grad_norm_epoch_svrg\\]\n",
    "- \\[grad_norm_epoch_saga\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d7c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GD')\n",
    "total_loss_epoch_gd, grad_norm_epoch_gd = net_gd.gd_backward(loss_function, n_epoch, lr_sgd)\n",
    "print('SGD')\n",
    "total_loss_epoch_sgd, grad_norm_epoch_sgd = net_sgd.sgd_backward(loss_function, n_epoch, lr_sgd)\n",
    "print('SVRG')\n",
    "total_loss_epoch_svrg, grad_norm_epoch_svrg = net_svrg.svrg_backward(loss_function, n_epoch, lr_svrg)\n",
    "print('SAGA')\n",
    "total_loss_epoch_saga, grad_norm_epoch_saga = net_saga.saga_backward(loss_function, n_epoch, lr_svrg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9ac22",
   "metadata": {},
   "source": [
    "## Objectif : Générer une visualisation.\n",
    "\n",
    "**Formules associées :**\n",
    "\n",
    "- \\[fmin\\]\n",
    "- \\[lw\\]\n",
    "- \\[lw\\]\n",
    "- \\[lw\\]\n",
    "- \\[lw\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60904a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin = np.min(total_loss_epoch_saga)\n",
    "plt.plot(range(n_epoch), total_loss_epoch_gd - fmin, lw=4, label = \"GD\")\n",
    "plt.plot(range(n_epoch), total_loss_epoch_sgd - fmin, lw=4, label = \"SGD\")\n",
    "plt.plot(range(n_epoch), total_loss_epoch_svrg - fmin, lw=4, label = \"SVRG\")\n",
    "plt.plot(range(n_epoch), total_loss_epoch_saga - fmin, lw=4, label = \"SAGA\")\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss_function - fmin')\n",
    "plt.title('objective functions evolution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c3fe2",
   "metadata": {},
   "source": [
    "## Objectif : Générer une visualisation.\n",
    "\n",
    "**Formules associées :**\n",
    "\n",
    "- \\[lw\\]\n",
    "- \\[lw\\]\n",
    "- \\[lw\\]\n",
    "- \\[lw\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca593000",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(n_epoch), grad_norm_epoch_gd, lw=4, label = \"GD\")\n",
    "plt.plot(range(n_epoch), grad_norm_epoch_sgd, lw=4, label = \"SGD\")\n",
    "plt.plot(range(n_epoch), grad_norm_epoch_svrg, lw=4, label = \"SVRG\")\n",
    "plt.plot(range(n_epoch), grad_norm_epoch_saga, lw=4, label = \"SAGA\")\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('grad norm')\n",
    "plt.title('grad norm evolution GD')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5bf66",
   "metadata": {},
   "source": [
    "## Objectif : Effectuer des calculs ou des opérations d'optimisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a82d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(total_loss_epoch_gd), np.min(total_loss_epoch_sgd), np.min(total_loss_epoch_svrg), np.min(total_loss_epoch_saga))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd44bba",
   "metadata": {},
   "source": [
    "## Objectif : Effectuer des calculs ou des opérations d'optimisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5daa427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
