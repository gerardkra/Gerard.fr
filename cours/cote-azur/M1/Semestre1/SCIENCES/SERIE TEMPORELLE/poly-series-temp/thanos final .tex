\documentclass[a4paper]{amsbook}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{xcolor}
\usepackage{graphicx}

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Comp}{\mathbb{C}}
\usepackage{bbm}
\newcommand{\var}{\mathbb{V}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\heta}{\hat{\eta}}
\newcommand{\hgamma}{\hat{\gamma}}
\newcommand{\Bor}{\mathcal{B}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Xtn}{\tilde{X}^n}
\newcommand{\clawn}{\underset{n\rightarrow
+\infty}{\Longrightarrow}} 
\newcommand{\convn}{\underset{n\rightarrow
+\infty}{\longrightarrow}} 
 \newcommand{\p}{\mathbb{P}}
\newcommand{\pperp}{\perp \! \! \! \perp}
\newcommand{\lam}{\lambda}
\newcommand{\convps}{\underset{n\rightarrow +\infty}{\overset{\text{p.s.}}{\longrightarrow}}}
\newcommand{\convpsm}{\underset{m\rightarrow +\infty}{\overset{\text{p.s.}}{\longrightarrow}}}
\newcommand{\convproba}{\underset{n\rightarrow +\infty}{\overset{\text{proba.}}{\longrightarrow}}}
\newcommand{\convloi}{\underset{n\rightarrow +\infty}{\overset{\text{loi}}{\longrightarrow}}}
\newcommand{\convlp}{\underset{n\rightarrow +\infty}{\overset{L^p}{\longrightarrow}}}
\newcommand{\convldeux}{\underset{n\rightarrow +\infty}{\overset{L^2}{\longrightarrow}}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\simU}{\sim \mathcal{U}([0;1])}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Id}{Id}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\cov}{Cov}

\newtheorem{mydef}{ Définition}

\newtheorem{defn} {Définition}
  \newtheorem{example}{Example} \theoremstyle{remark}
  \newtheorem{rem}{Rémark}
  \theoremstyle{plain}
  \newtheorem{prop}{Proposition}
  \theoremstyle{plain}
  \newtheorem{lem}{Lemma}
  \theoremstyle{plain}
  \newtheorem{fact}{Fait}
  \theoremstyle{plain}
  \newtheorem{cor}{Corollaire}




\begin{document}


\chapter{Modélisation des séries stationnaires}

\section{Auto-corrélation partielle\index{Auto-corrélation partielle}}

\begin{defn}
	
Pour un processus stationnaire $(X_{t})$, le coefficient de corrélation
partielle (on dit aussi corrélation partielle) entre $X_{n}$ et $X_{1}$
est 
\[
r_{X_{2},\dots,X_{n}}(X_{1},X_{n})=\frac{1}{\sigma(0)}\cov(X_{1}-\E(X_{1}|X_{2},\dots,X_{n-1}),X_{n}-\E(X_{n}|X_{2},\dots,X_{n}))\,,
\]
le coefficient d'auto-corrélation partielle en $h$ est défini par
\[
\begin{cases}
r(h)=r_{X_{2},\dots,X_{h-1}}(X_{1},X_{h}) & \mbox{ pour }h\geq2\mbox{,}\\
r(1)=\rho(1)\mbox{,}\\
r(0)=1\mbox{,}\\
r(-h)=r(h) & \mbox{ pour }h\leq-1\mbox{.}
\end{cases}
\]
 Pour un processus stationnaire,
\[
r(h)=r_{X_{n+1},\dots,X_{n+h-1}}(X_{n},X_{n+h})\mbox{ pour tout }n\mbox{.}
\]
Donc on va pouvoir estimer ces coefficients en faisant des moyennes
empiriques (c'est ce que fait la fonction \texttt{pacf }de \texttt{R})
\end{defn}


\section{Les processus linéaires générales}

Soit $\{Y_t \}$ une série que est observée et une série de bruit $\{e_t \}$ qui n’est pas observée. Le bruit satisfait les hypothèses classiques. 

Nous voudrions écrire  $\{Y_t \}$ comme une combinaison linéaire, c’est-à-dire que \textit{ nous expliquons  la série observée par les perturbations aléatoires passées}. 

\begin{equation}
\label{eq: gen lin}
	Y_t=e_t+\psi_1e_{t-1}+\psi_2 e_{t-2}+\cdots 
\end{equation}
Si c’est une somme infinie, conditions spécifiques doivent être placés sur les coefficients pour que la somme soit convergeant.  

\begin{example}
	
Soit le cas que \ref{eq: gen lin} soit sur la forme 

\begin{equation}
\label{eq: q-lin}
	Y_t=e_t+\psi e_{t-1}+\psi^2 e_{t-2}+\cdots =\sum_{i=0}^\infty\psi^ie_{t-i} \quad\psi^0=1
\end{equation}


Ici, la condition pour \ref{eq: q-lin} soit convergeant est $|\psi|<1$
\begin{itemize}
	\item $\E(Y_t)=0 $
	\item $Var(Y_t)= \frac{\sigma_e^2}{1-\psi^2}$
	\item $Cov(Y_t,Y_{t-1})= \frac{ \psi \sigma_e^2}{1-\psi^2} $ et $Cov(Y_t,Y_{t-k})= \frac{ \psi^k \sigma_e^2}{1-\psi^2} $ 
	\item $Corr(Y_t,Y_{t-1})=\psi$ et  $Corr(Y_t,Y_{t-k})=\psi^k$
\end{itemize}
La processus est stationnaire, l’autocovariance ne dépend pas au temps $t$, seulement à décalage du temps (lag) k 

\end{example}


Avant de nous lancer dans le reste de la section, nous
avons besoin de définir quelques notions. On définit l'opérateur de
décalage sur les suites stationnaires:

\[
L\,:\,(Y_{t})_{t\geq0}\mapsto\left(L(Y)_{t}=\begin{cases}
Y_{t-1} & \mbox{ si }t\geq1\\
0 & \mbox{ si }t=0
\end{cases}\right)_{t\geq0}
\]
(ici, $(Y_{t})$ est une suite stationnaire). Pour un polynôme $P\in\mathbb{C}[X]$
($P(X)=\alpha_{0}+\alpha_{1}X+\dots+\alpha_{n}X$, $n$ étant le degré
de $P$), nous définissons l'opérateur $P(L)$ par~:
\[
P(L)\,:\,(Y_{t})_{t\geq0}\mapsto(P(L)(Y)_{t})\text{ avec }P(L)(Y)_{t}=\begin{cases}
\alpha Y_{t}+\alpha_{1}Y_{t-1}+\dots+\alpha_{n}Y_{t-n} & \text{ si }t\geq n\,,\\
0 & \text{ sinon.}
\end{cases}
\]
Nous remarquons que pour deux polynômes $P$ et Q~:
\[
P(L)Q(L)=(PQ)(L)\,,
\]
où le produit de gauche est la composition. 
\begin{lem}
Pour $\lambda$ un nombre complexe de module $<1$, l’opérateur $1-\lambda L$
admet pour inverse
\[
(1-\lambda L)^{-1}=\sum_{k=0}^{+\infty}(\lambda L)^{k}\,.
\]
\end{lem}
\begin{proof}
Pour toute suite stationnaire $(Y_{t})_{t\geq0}$, nous regardons
à $t$ fixé~:
\[
\sum_{k=0}^{+\infty}\left((\lambda L)^{k}Y\right)_{t}=\sum_{k=0}^{t}\lambda^{k}Y_{t-k}\,,
\]
qui est une somme finie. Donc la suite 
\[
\left(\sum_{k=0}^{+\infty}(\lambda L)^{k}Y\right)_{t\geq0}
\]
est bien définie. Nous avons
\begin{multline*}
Y=(Y_{0},Y_{1},\dots)\overset{\sum_{k=0}^{+\infty}(\lambda L)^{k}}{\longmapsto}(Y_{0},Y_{1}+\lambda Y_{0},Y_{2}+\lambda Y_{1}+\lambda^{2}Y_{2},\dots)\\
\overset{1-\lambda L}{\longmapsto}(Y_{0},(Y_{1}+\lambda Y_{0})-\lambda Y_{0},(Y_{2}+\lambda Y_{1}+\lambda^{2}Y_{2})-\lambda(Y_{1}+\lambda Y_{0})-\lambda^{2}Y_{0},\dots)\\
=(Y_{0},Y_{1},Y_{2},\dots)\,.
\end{multline*}
Donc $(1-\lambda L)\sum_{k=0}^{+\infty}(\lambda L)^{k}=\Id$. On montre
facilement de même que $\sum_{k=0}^{+\infty}(\lambda L)^{k}(1-\lambda L)=\Id$.
\end{proof}


\section{Les processus auto-régressifs }
\begin{defn}

Un processus $(X_{t})$ est dit auto-régressif\index{Processus auto-régressif}
d'ordre $p$ centré s'il vérifie
\begin{equation}
X_{t}=\epsilon_{t}+\sum_{j=1}^{p}a_{j}X_{t-j}\,,\,\mbox{pour tout }t\geq0\label{eq:ARp}
\end{equation}
(avec $p\in\N^{*}$, $a_{p}\neq0$) avec des $\epsilon_{t}$ qui forment
un bruit blanc centré de variance $\sigma^{2}$ ($p\in\N^{*}$, $a_{1},\dots,a_{p}\in\R$),
tels que $\epsilon_{t}$ est indépendant de $X_{t-1}$, $X_{t-2}$,
\dots{} pour tout $t$. Par convention~: $X_{-k}=0$ pour tout $k$
dans $\N^{*}$. On dira aussi que $(X_{t})$ est un processus $AR(p)$. 

On dit que le $X_{t}$  "s'explique " par les $p$ observations
précédentes ($X_{t-1}, X_{t-2}, \cdots , X_{t-p}$). Dans cette
définition, le processus $(\epsilon_{t})$ s'appelle processus des
innovations\index{Innovations}. 
\end{defn}

\begin{example}{AR(1)}
	\begin{equation}
\label{eq: AR1}
	Y_t=a Y_{t-1}+e_t
\end{equation}
\textbf{Nous supposons que la série est stationnaire, et de moyenne zéro} 
\begin{itemize}
	\item $Var(Y_t)=\frac{\sigma_e^2}{1-a^2} $
	\item $Cov(Y_t,Y_{t-1})=  \frac{a \sigma_e^2}{1-a^2}  $ et  $Cov(Y_t,Y_{t-k})= a^k \frac{\sigma_e^2}{1-a^2}  $
	\item $Corr (Y_t,Y_{t-1})=a $ et $Corr (Y_t,Y_{t-k})=a^k $
\end{itemize}
\begin{rem}
De la variance on pourrais suspecter la condition $|a|<1$ mais pour le justifier rigoureusement on doit d’aller plus loin sur les propriétés d’un modèle AR. 
	
\end{rem}

\textbf{L’inversion d’un modèle AR(1) et la condition pour la stationnarité }

Nous rendrons le modèle AR(1) sous la forme récursif (c’est à dire,  nous chercherons à faire la recursion sur les valeurs précédents de la série)
\begin{align*}
	Y_t=& a Y_{t-1}+e_t= e_t + a \underbrace{(a Y_{t-2}+e_{t-1}) }_{Y_{t-1}} 
\end{align*}
Si nous répétons la substitution $k-1$ fois, nous arrivons à
\begin{equation}
		\label{eq: rec AR}
		Y_t=e_t+a e_{t-1}+a^2 e_{t-2}+\cdots + a^{k-1} e_{t-k+1}+ a^k e_{t-k}
\end{equation}
Ce qui resemble à le modèle \ref{eq: q-lin}, et en plus, si nous laisserions $k\to\infty$,  alors nous récupérons modèle  \ref{eq: q-lin} et la condition pour la stationnarité. 

\end{example}

\begin{example}{AR(2)} 
	\begin{equation}
	\label{eq: AR2}
	Y_t= a_1Y_{t-1}+a_2Y_{t-2}+e_t
\end{equation}
en utilisant l' opérateur de décalage  $L$, avec $L(Y_t)=Y_{t-1}$ et en general $L^k(Y_t)=Y_{t-k}$  alors \ref{eq: AR2} pourrait être écrit: 
\begin{equation}
	\label{eq: LAR2}
	(1-a_1L-a_2L^2)Y_t=e_t
\end{equation}
par \ref{eq: LAR2} on appelle polynôme caractéristique: 
\begin{equation}
	\label{eq: pol char}
	1-a_1z-a_2z^2=0
\end{equation}  

Pour que  $(1-\phi_1L-\phi_2L^2) $ soit invertible, il faut placer les conditions sur les racines de \ref{eq: pol char}. Après, changer le côté de \ref{eq: LAR2}
\begin{align}
\label{psiL}
&\psi(L)=\frac{1}{1-a_1L-a_2L^2}\\
\label{AR noise}
&Y_t=\psi(L)e_t
\end{align}
Alors pour inverser le modèle AR à MA on a besoin de developer \ref{psiL} aux puissants de $L$
par la série géométrique,

\begin{equation*}
	\frac{1}{1-a_1L-a_2L^2}=1+a_1L+a_2L^2+\cdots
\end{equation*} 

Ainsi de \ref{AR noise}
\begin{equation}
	\label{eq: AR2 inv}
	Y_t= e_t+a_1e_{t-1}+a_2 e_{t-2}+\cdots
\end{equation}
Nous arrivons à la forme inversée infini. Pour preciser la forme des coefficients $\psi_i$ à \ref{eq: AR2 inv}  , soit $r_1,r_2$ les racines du polynôme caractéristique et $R_{1,2}=\frac{1}{r_ {1,2}}$ , on a la factorisation 

\begin{equation*}
	\frac{1}{1-a_1z-a_2z^2}=	\frac{1}{(1-R_1z)(1-R_2z)}= \frac{A}{1-R_1z}+\frac{B}{1-R_2z}
\end{equation*}
  
  \begin{equation*}
  	 A= \frac{r_2}{r_2-r_2} \quad B= \frac{-r_1}{r_2-r_1}
  \end{equation*}
  
\begin{equation}
\label{eq: inversion pol}
	\frac{1}{1-a_1z-a_2z^2} =\sum_{i=0}^\infty (AR_1^i+BR_2^i)z^i 
\end{equation}

C’est facile de verifier par \ref{eq: inversion pol} que $\psi_0=1$ 

Par cette derivation, il est clair que on a besoin d’ étudier les racines du polynôme caractéristique. 


\textbf{Etude des racines du polynôme caractéristique}


Pour AR(2) d’être invertible et alors stationnaire il faut que ses racines sont hors du circle unitaire.  Nous montrerons quelques conditions pour les coefficients de \ref{eq: AR2} 
Nous savons que. 
\begin{equation*}
	r_{1,2}=\frac{a_2  \sqrt{a_1+4 a_2}}{-2 a_2}
\end{equation*}  
Et 
\begin{equation*}
	R_1=\frac{1}{r_2}=\frac{a_1-\sqrt{a_1+4 a_2}}{2}\quad 	R_2=\frac{a_1+\sqrt{a_1+4 a_2}}{2}
\end{equation*}

Par \ref{eq: inversion pol} est claire qu’il faut  $|R_i|<1$ for $i=1,2$ 
\begin{description}
	\item{\textbf{ Racines reels } } 
	Pour avoir deux solutions reels il faut $a_1^2+4a_2\geq0$
	\begin{equation*}
		-1<\frac{a_1-\sqrt{a_1+4 a_2}}{2}<\frac{a_1+\sqrt{a_1+4a_2}}{2}<1
	\end{equation*}
	en faisant les calcules nécessaires on arrive à les conditions 
	\begin{equation*}
		a_2-a_1<1\quad et\quad a_2+a_1<1
	\end{equation*}   
	\item{\textbf{Racines complexes }} 	Pour avoir deux solutions complexes il faut $a_1^2+4a_2<0$
	Ici, les $R_{1,2}$ sont complex conjugates et 
	\begin{equation*}
		|R_1|=|R_2|<1 \Leftrightarrow |R_1|^2<1	
	\end{equation*}
	\begin{equation*}
		|R_1|^2=\frac{a_1^2+(-a_1^2-4 a_2)}{4}=-a_2
	\end{equation*}
	Et alors $|a_2|<1$
\end{description}

\end{example}




\begin{prop}
\label{prop:existence-AR-stationnaire}On associe le polynôme de $\R[X]$
suivant à l'équation (\ref{eq:ARp})
\[
A(X)=1-a_{1}X-\dots-a_{p}X^{p}\,.
\]
Si les racines (dans $\mathbb{C}$) de ce polynôme sont toutes de
module strictement supérieur à $1$ alors il existe un processus stationnaire
$(X_{t})$ vérifiant (\ref{eq:ARp}) et tel que $\epsilon_{t}$ est
le bruit d'innovation \index{Bruit d'innovation} pour ce processus
(c'est à dire que, pour tout $t$, $\epsilon_{t}$ est indépendant
de $X_{t-1},X_{t-2},\dots$).
\end{prop}
\begin{rem}
La condition " racines de $A$ de module $>$ 1 "  n'est pas
nécessaire à l'existence d'une suite stationnaire vérifiant (\ref{eq:ARp}).
Voir le chapitre 5 de l'édition en anglais de \cite{gourieroux-montfort-1997}
pour plus de détails.
\end{rem}

\begin{proof}
On peut toujours développer la fraction rationnelle $1/A(z)$ ($z\in\mathbb{C}$)
dans un voisinage de $0$
\begin{multline*}
\frac{1}{A(z)}=\frac{1}{1-a_{1}z-\dots-a_{p}z^{p}}=1-(a_{1}z+\dots+a_{p}z^{p})-(a_{1}z+\dots+a_{p}z^{p})^{2}-\dots\\
=1+\alpha_{1}z+\alpha_{2}z^{2}+\dots
\end{multline*}
pour certains coefficients $\alpha_{1}$, $\alpha_{2}$, \dots{} (on
remarque que ces coefficients sont dans $\R$). Plus précisément,
nous avons pour tout $z$, 
\[
\frac{1}{A(z)}=\prod_{i=1}^{p}\left(\frac{1}{1-u_{i}z}\right),
\]
où l'ensemble des racines de $A$ est $\{1/u_{1},\dots,1/u_{p}\}$
(les racines apparaissent avec leur multiplicité). Donc, pour $z$
tel que $|z|<\inf_{i}(1/|u_{i}|)$, nous pouvons développer en produit
de séries entières
\[
\frac{1}{A(z)}=\prod_{i=1}^{p}\left(\sum_{k=0}^{+\infty}u_{i}^{k}z^{k}\right)\,.
\]
Quand on développe le produit ci-dessus, le coefficient de $z^{n}$
est 
\[
\alpha_{n}=\sum_{\begin{array}{c}
j_{1},\dots,j_{p}\geq0\\
j_{1}+\dots+j_{p}=n
\end{array}}u_{1}^{j_{1}}\dots u_{p}^{j_{p}}\,.
\]
Donc 
\begin{equation}
|\alpha_{n}|\leq(\sup_{i}|u_{i}|)^{n}\frac{(n+1)^{p}}{p!}\label{eq:maj-alpha}
\end{equation}
(nous utilisons~: $\#\{(j_{1},\dots,j_{p})\in(\N)^{p}\,:\,j_{1}+\dots+j_{p}=n\}=\frac{(n+1)^{p}}{p!}$
, qui nécessite un petit calcul). 

On suppose que l'on dispose d'un bruit blanc $(\epsilon_{k})_{k\in\Z}$.
On fixe $t$ dans $\N$ et on s'intéresse à la suite $(Y_{k}=\epsilon_{t}+\alpha_{1}\epsilon_{t-1}+\dots+\alpha_{k}\epsilon_{t-k})_{k\geq1}$.
La série $\sum_{n\geq1}|\alpha_{n}|$ est convergente (petit exercice
sur la convergence des séries, c'est ici que l'on utilise que $\sup_{i}|u_{i}|<1$)
donc la série $\sum_{n\geq1}\alpha_{n}^{2}$ aussi. En particulier,
pour tout $\delta>0$, il existe $N$ dans $\N$ tel que 
\[
n\geq N\Rightarrow\forall k\geq0\,,\,\alpha_{n}^{2}+\alpha_{n+1}^{2}+\dots+\alpha_{n+k}^{2}<\delta\,.
\]
Pour tout $n>N$ et $k\geq0$, nous avons alors 
\begin{eqnarray*}
\E((Y_{n}-Y_{n+k})^{2}) & = & \E((\alpha_{n+1}\epsilon_{t-n-1}+\dots+\alpha_{n+k}\epsilon_{t-n-k})^{2})\\
 & = & (\alpha_{n+1}^{2}+\dots+\alpha_{n+k}^{2})\sigma^{2}\\
 & < & \delta
\end{eqnarray*}
Donc la suite $(Y_{n})_{n\geq1}$ est une suite de Cauchy dans $L^{2}(\p)$
(l'espace des variables aléatoire réelles de carré intégrable, muni
de la norme $L^{2}$). Comme cette espace est complet, la suite $(Y_{n})_{n\geq1}$
converge dans $L^{2}$ vers une limite que nous noterons $X_{t}$.
Nous pouvons écrire
\begin{equation}
X_{t}=\epsilon_{t}+\alpha_{1}\epsilon_{t-1}+\alpha_{2}\epsilon_{t-2}+\dots\label{eq:AM-ordre-infini}
\end{equation}
 (en considérant le terme de droite comme une limite dans $L^{2}$).

Nous aimerions montrer que cette limite a aussi lieu p.s. Soit $\delta>0$.
For all $n\geq1$, 
\begin{eqnarray*}
\p(|Y_{n}-X_{t}|\geq\delta) & = & \p((\alpha_{n+1}\epsilon_{t-n-1}+\alpha_{n+2}\epsilon_{t-n-1}+\dots)^{2}\geq\delta^{2})\\
\mbox{(inégalité de Markov)} & \leq & \frac{\E((\alpha_{n+1}\epsilon_{t-n-1}+\alpha_{n+2}\epsilon_{t-n-2}+\dots)^{2})}{\delta^{2}}\\
 & = & \frac{\sigma^{2}(\alpha_{n+1}^{2}+\alpha_{n+2}^{2}+\dots)}{\delta^{2}}\,.
\end{eqnarray*}
Et donc
\begin{eqnarray*}
\sum_{n\geq1}\p(|Y_{n}-X_{t}|\geq\delta) & \leq & \sum_{n\geq1}\frac{\sigma^{2}(\alpha_{n+1}^{2}+\alpha_{n+2}^{2}+\dots)}{\delta^{2}}\\
 & = & \frac{\sigma^{2}}{\delta^{2}}\sum_{n\geq2}(n-1)\alpha_{n}^{2}\\
 & < & \infty
\end{eqnarray*}
(petit exercices sur les séries à partir de l'équation (\ref{eq:maj-alpha})).
Le lemme de Borel-Cantelli nous dit que, p.s., il existe un $N$ tel
que, pour $n>N$, $|Y_{n}-X_{t}|<\delta$. Ceci est valable pour tout
$\delta>0$, donc (attention, nous sautons une étape de la démonstration)
$Y_{n}\convps X_{t}$.

Montrons que les $X_{t}$ que nous venons de définir vérifient la
relation de récurrence (\ref{eq:ARp}). Nous avons, pour tout $t$,
\begin{eqnarray*}
X_{t}-a_{1}X_{t-1}-\dots-a_{p}X_{t-p} & = & \epsilon_{t}+\alpha_{1}\epsilon_{t-1}+\alpha_{2}\epsilon_{t-2}+\dots\\
 &  & -a_{1}(\epsilon_{t-1}+\alpha_{1}\epsilon_{t-2}+\alpha_{2}\epsilon_{t-3}+\dots)\\
 &  & -\dots\\
 &  & -a_{p}(\epsilon_{t-p}+\alpha_{1}\epsilon_{t-p-1}+\alpha_{2}\epsilon_{t-p-2}+\dots)\\
\mbox{(convention }\alpha_{0}=1\mbox{)} & = & \epsilon_{t}+\epsilon_{t-1}(\alpha_{1}-a_{1})+\epsilon_{t-2}(\alpha_{2}-a_{1}\alpha_{1}-a_{3})\\
 &  & +\dots\\
 &  & +\epsilon_{t-k}(\alpha_{k}-\sum_{1\leq i\leq k\wedge p}\alpha_{k-i}a_{i})\\
 &  & +\dots
\end{eqnarray*}
Nous avons, pour tout $z$ dans un voisinage adéquat de $0$, 
\begin{eqnarray*}
1 & = & A(z)\times\frac{1}{A(z)}\\
 & = & (1-a_{1}z-\dots-a_{p}z^{p})\times(1+\alpha_{1}z+\alpha_{2}z^{2}+\dots)\,.
\end{eqnarray*}
Si nous développons ce dernier produit, nous trouvons une série entière
dans laquelle le coefficient de $z^{n}$ ($n\geq1$) est 
\[
\alpha_{n}-\sum_{1\leq i\leq n\wedge p}\alpha_{k-i}a_{i},
\]
qui doit être nul puisque la série vaut $1$. Donc 
\[
X_{t}-a_{1}X_{t-1}-\dots-a_{p}X_{t-p}=\epsilon_{t}\,.
\]
On montre facilement que la suite $(X_{t})_{t\geq1}$ est sationnaire
(exercice) (voir la définition \ref{def:processus-stationnaire}).
\end{proof}
\begin{prop}
S'il existe un processus stationnaire $(X_{t})_{t\geq0}$ satisfaisant
la relation de récurrence de l'équation (\ref{eq:ARp}) alors sa fonction
d'auto-covariance vérifie~:
\begin{equation}
\text{pour \ensuremath{h>0}, }\sigma(h)=a_{1}\sigma(h-1)+a_{2}\sigma(h-2)+\dots+a_{p}\sigma(h-p)\,,\label{eq:rec-cov-AR}
\end{equation}
\[
\text{et }\sigma(0)=\sigma^{2}+a_{1}\sigma(1)+a_{2}\sigma(2)+\dots+a_{p}\sigma(p)\,,
\]

et sa fonction d'auto-corrélation vérifie 
\[
\text{pour }h>1\text{, }\rho(h)=a_{1}\rho(h-1)+a_{2}\rho(h-2)+\dots+a_{p}\rho(h-p)\,.
\]
\end{prop}
\begin{proof}
Nous avons pour tout $t$ et tout $h\geq1$, 
\begin{eqnarray*}
\cov(X_{t}X_{t+h}) & = & \E(X_{t}X_{t+h})\\
\sigma(h) & = & \E(X_{t}(\epsilon_{t+h}+\sum_{i=1}^{p}a_{i}X_{t+h-i}))\\
\sigma(h) & = & 0+\sum_{i=1}^{p}a_{i}\sigma(h-i)
\end{eqnarray*}
et
\begin{eqnarray*}
\sigma(0)=\var(X_{t}) & = & \E(X_{t}(\epsilon_{t}+\sum_{i=1}^{p}a_{i}X_{t-i}))\\
 & = & \E(X_{t}\epsilon_{t})+\sum_{i=1}^{p}a_{i}\sigma(i)\\
 & = & \E((\epsilon_{t}+\sum_{i=1}^{p}a_{i}X_{t-i})\epsilon_{t})+\sum_{i=1}^{p}a_{i}\sigma(i)\\
\text{(car }\epsilon_{t}\text{ ind. de }X_{j}\text{ pour }j<t\text{)} & = & \sigma^{2}+\sum_{i=1}^{p}a_{i}\sigma(i)\,.
\end{eqnarray*}
\end{proof}
Le polynôme caractéristique de la relation de récurrence de l'équation
(\ref{eq:rec-cov-AR}) est
\begin{eqnarray*}
B(X)=X^{p}-a_{1}X^{p-1}-\dots-a_{p} & = & X^{p}\left(1-\frac{a_{1}}{X}-\dots-\frac{a_{p}}{X^{p}}\right)\\
 & = & X^{p}A\left(\frac{1}{X}\right)\,.
\end{eqnarray*}
Donc, si les racines de $A$ sont de modules $>1$, alors celle de
$B$ sont de module $<1$. Si les racines de $B$ sont distinctes
égales à $\lambda_{1},\lambda_{2},\dots,\lambda_{p}$, les solutions
de (\ref{eq:rec-cov-AR}) sont de la forme
\[
\sigma(h)=\sum_{i=1}^{p}c_{i}\lambda_{i}^{h}
\]
(voir \cite{gourdon-2008}, p. 196 pour plus de détails). On en déduit
le résultat suivant.
\begin{lem}
S'il existe un processus stationnaire $(X_{t})_{t\geq0}$ satisfaisant
la relation de récurrence de l'équation (\ref{eq:ARp}) et si le polynôme
de la relation de récurrence a des racines qui sont toutes de module
$>1$, alors la fonction d'auto-covariance du processus décroît exponentiellement
quand $h\rightarrow+\infty$ (et donc sa fonction d'auto-corrélation
a le même comportement).
\end{lem}
\begin{prop}
S'il existe un processus stationnaire $(X_{t})_{t\geq0}$ satisfaisant
la relation de récurrence de l'équation (\ref{eq:ARp}) alors sa fonction
d'auto-corrélation partielle vérifie~:
\[
r(h)=0\text{ si }h\geq p+1\,.
\]
\end{prop}
\begin{proof}
Soit $h\geq p+1$. Nous calculons pour $t$ quelconque~:
\begin{eqnarray*}
\E(X_{t+h}|X_{t+1},\dots,X_{t+h-1}) & = & \E(\epsilon_{t+h}+a_{1}X_{t+h-1}+\dots+a_{p}X_{t+h-p}|X_{t+1},\dots,X_{t+h-1})\\
 & = & a_{1}X_{t+h-1}+\dots+a_{p}X_{t+h-p}\,,
\end{eqnarray*}
\[
X_{t+h}-\E(X_{t+h}|X_{t+1},\dots,X_{t+h-1})=\epsilon_{t+h}\,,
\]
\begin{multline*}
\E(X_{t}|X_{t+1},\dots,X_{t+h-1})=\\
\E\left(-\frac{1}{a_{p}}(X_{t+p}-\epsilon_{t+p}-a_{1}X_{t+p-1}-\dots-a_{p-1}X_{t+1})|X_{t+1},\dots,X_{t+h-1}\right)=\\
-\frac{1}{a_{p}}(X_{t+p}-a_{1}X_{t+p-1}-\dots-a_{p-1}X_{t+1})\,,
\end{multline*}
\[
X_{t}-\E(X_{t}|X_{t+1},\dots,X_{t+h-1})=\frac{\epsilon_{t+p}}{a_{p}}\,.
\]
Donc
\[
\cov(X_{t+h}-\E(X_{t+h}|X_{t+1},\dots,X_{t+h-1}),X_{t}-\E(X_{t}|X_{t+1},\dots,X_{t+h-1}))=0\,.
\]
\end{proof}

\section{Les processus en moyenne mobile\index{Processus en moyenne mobile}}
\begin{defn}
Un processus en moyenne mobile d'ordre $q$ est un processus de la
forme 
\begin{equation}
X_{t}=\epsilon_{t}+b_{1}\epsilon_{t-1}+\dots+b_{q}\epsilon_{t-q}\,,\,\mbox{pour tout }t\geq0\label{eq:MA(q)}
\end{equation}
 (avec $q\in\N^{*}$, $b_{q}\neq0$) avec des $(\epsilon_{t})$ qui
forment un bruit blanc centré de variance $\sigma^{2}$. Par convention~:
$\epsilon_{-k}=0$ pour tout $k$ dans $\N^{*}$. On dira aussi que
$(X_{t})$ est un processus $MA(q)$.
\end{defn}
Pour un bruit blanc centré $(\epsilon_{t})$, un tel processus existe
toujours et est toujours stationnaire.





\begin{prop}
L'auto-covariance d'un processus $MA(q)$ vérifiant l'équation (\ref{eq:MA(q)})
vérifie, pour $h\geq0$, 
\[
\sigma(h)=\begin{cases}
\sigma^{2}\sum_{k=0}^{q-h}b_{k}b_{k+h} & \mbox{ si }h\leq q\,,\\
0 & \mbox{ si }h>q\,,
\end{cases}
\]
sous la convention $b_{0}=1$.
\end{prop}
\begin{proof}
Nous calculons
\begin{eqnarray*}
\E(X_{t}X_{t+h}) & = & \E\left(\left(\sum_{k=0}^{q}\epsilon_{t-k}b_{k}\right)\times\left(\sum_{k=0}^{q}\epsilon_{t+h-k}b_{k}\right)\right)\\
 & = & \begin{cases}
0 & \mbox{ si }h>q\,,\\
\sum_{k=q}^{q-h}\E(\epsilon_{t-k}b_{k}\epsilon_{t-k}b_{k+h}) & \mbox{ sinon .}
\end{cases}
\end{eqnarray*}
\end{proof}
De la proposition \ref{prop:existence-AR-stationnaire} et de l'équation
(\ref{eq:AM-ordre-infini}), nous déduisons le corollaire suivant.
\begin{cor}
Sous les hypothèses de la proposition \ref{prop:existence-AR-stationnaire},
le processus $AR(p)$ stationnaire vérifiant l'équation de récurrence
(\ref{eq:ARp}) peut s'écrire comme un processus $AM$ d'ordre infini
(au sens de l'équation (\ref{eq:AM-ordre-infini}) (dans laquelle
la limite a lieu p.s. et dans $L^{2}$).
\end{cor}
La démonstration du résultat suivant étant similaire à la démonstration
de la proposition \ref{prop:existence-AR-stationnaire}, nous le citons
comme un simple corollaire. 
\begin{cor}
Si $(X_{t})_{t\geq0}$ est un processus $AM(q)$ satisfaisant l'équation
(\ref{eq:MA(q)}) et si le polynôme $B(X)=1+b_{1}X+\dots+b_{q}X^{q}$
n'admet que des racines de module $>1$ , alors $(X_{t})$ peut s'écrire
comme un processus $AR$ d'ordre infini et tel que $\epsilon_{t}$
est le bruit d'innovation pour ce processus.\\
En particulier, sa fonction d'auto-corrélation partielle vérifie~:
\[
r(h)\underset{h\rightarrow+\infty}{\longrightarrow}0\,.
\]
\end{cor}
En effet, en partant de l'équation (\ref{eq:MA(q)}), on peut exprimer
$\epsilon_{t}$ en fonction de $X_{t}$, $X_{t-1}$, \dots{} pour
tout $t$ (de même qu'en partant de l'équation (\ref{eq:ARp}), on
peut exprimer $X_{t}$ en fonction de $\epsilon_{t}$, $\epsilon_{t-1}$,
\dots{} pour tout $t$). Pour plus de détail voir \cite{gourieroux-montfort-1997}
(ces questions sont développées au chapitre 5 de l'édition en anglais
de ce livre).

\section{Les processus mixtes ARMA(p,q). }
\begin{defn}
Un processus auto-régressif en moyenne mobile\index{Processus auto-régressif en moyenne mobile}
d'ordres $p$, $q$ (tels que $p\times q\neq0$) est un processus
qui peut s'écrire 
\begin{equation}
X_{t}=\sum_{k=1}^{p}a_{k}X_{t-k}+\sum_{j=0}^{q}b_{j}\epsilon_{t-j}\,,\,\mbox{pour tout }t\geq0\,,\label{eq:def-ARMA}
\end{equation}
où les $(\epsilon_{j})$ sont des bruits blancs centrés de variance
$\sigma^{2}$. Par convention~: $X_{-k}=\epsilon_{-k}$ pour tout
$k$ dans $\N^{*}$. On dira aussi que $(X_{t})$ est un processus
$ARMA(p,q)$. 

On peut toujours écrire 
\[
X_{t}-a_{1}X_{t-1}-\dots-a_{p}X_{t-p}=\epsilon_{t}+b_{1}\epsilon_{t-1}+\dots+b_{q}\epsilon_{t-q}\,,
\]
d'où la relation
\begin{eqnarray*}
\cov(X_{t+h}-a_{1}X_{t+h-1}-\dots-a_{p}X_{t+h-p},X_{t}) & = & \sigma(h)-a_{1}\sigma(h-1)-\dots-a_{p}\sigma(h-p)\\
 & = & \cov(\epsilon_{t+h}+b_{1}\epsilon_{t+h-1}+\dots+b_{q}\epsilon_{t+h-q},X_{t})\,.
\end{eqnarray*}
Cette dernière quantité est nulle dès que $h>q$. Les auto-covariances
d'un processus $ARMA(p,q)$ vérifie donc la même relation de récurrence
que celles d'un processus $AR(p)$ à partir d'un certain rang. Donc
ces auto-covariances (ainsi que les auto-corrélations correspondantes)
convergent exponentiellement vite vers $0$ à l'infini.
\end{defn}
\begin{prop}
\label{prop:racines-communes}On s'intéresse à la relation de récurrence
(\ref{eq:def-ARMA}) ci-dessus. On définit les polynômes
\[
A(X)=1-a_{1}X-\dots-a_{p}X^{p}\,,\,B(X)=1+b_{1}X+\dots+b_{q}X^{q}\,.
\]
Si les racines de $A$ et $B$ sont de modules $>1$, alors il existe
un processus stationnaire vérifiant la relation (\ref{eq:def-ARMA})
et tel que $\epsilon_{t}$ est le bruit d'innovation pour ce processus.

Si les racines de $A$ et $B$ sont de module $>1$ alors : " $(X_{t})$
ne vérifie pas de relation de récurrence plus courte que (\ref{eq:def-ARMA}) "
implique " $A$ et $B$ n'ont pas de racine commune ". Si
on suppose, de plus, que les $(\epsilon_{t})$ on des densités par
rapport à la mesure de Lebesgue, alors les deux propositions sont
équivalentes. 
\end{prop}
\begin{rem}
Pour plus de détail voir \cite{gourieroux-montfort-1997} (ces questions
sont développées au chapitre 5 de l'édition en anglais de ce livre).
\end{rem}
\begin{proof}[Démonstration partielle de la proposition \ref{prop:racines-communes}]Supposons
que $A$ et $B$ n’ont que des racines de module $>1$. Supposons,
de plus, que $A$ et $B$ ont une racine commune, nous allons montrer
que $(X_{t})$ et $(\epsilon_{t})$ vérifie une relation plus courte
que (\ref{eq:def-ARMA}). Notons $1/\lambda$ une racine commune à
$A$ et $B$. Nous avons donc $p\geq1$ et $q\geq1$. Nous pouvons
écrire 
\[
A(X)=(1-\lambda X)A_{1}(X)\,,\,B(X)=(1-\lambda X)B_{1}(X)
\]
avec $A_{1}$ polynôme de degré $p-1$ et $B_{1}$ polynôme de degré
$q-1$. Nous avons 
\begin{eqnarray*}
A(L)X & = & B(L)\epsilon\\
(1-\lambda L)A_{1}(L)X & = & (1-\lambda L)B_{1}(L)\epsilon\\
(1-\lambda L)^{-1}(1-\lambda L)A_{1}(L)X & = & (1-\lambda L)^{-1}(1-\lambda L)B_{1}(L)\epsilon\\
A_{1}(L)(X) & = & B_{1}(L)\epsilon\,.
\end{eqnarray*}

\end{proof}
\begin{example}
Soient 
\[
A(X)=\left(1-\frac{X}{2}\right)\left(1-\frac{X}{3}\right)\,,\,B(X)=\left(1-\frac{X}{2}\right)\left(1+\frac{X}{4}\right)\,.
\]
Nous avons 
\begin{eqnarray*}
A(X) & = & 1-\frac{5}{6}X+\frac{X^{2}}{6}\,,\\
B(X) & = & 1-\frac{X}{4}-\frac{X^{2}}{8}\,.
\end{eqnarray*}
On s’intéresse au processus $ARMA(2,2)$ (stationnaire) vérifiant
la relation de récurrence~:
\begin{equation}
X_{t}-\frac{5}{6}X_{t-1}+\frac{1}{6}X_{t-2}=\epsilon_{t}-\frac{1}{4}\epsilon_{t-1}-\frac{1}{8}\epsilon_{t-2}\,.\label{eq:rec-01}
\end{equation}
Les polynômes caractéristiques de cette relation sont les $A$, $B$
ci-dessus. Ils ont une racine commune ($2$). Nous allons montrer
que $(X_{t})$, $(\epsilon_{t})$ vérifient une relation de récurrence
plus courte que (\ref{eq:rec-01}).Soit la suite 
\begin{eqnarray*}
(Y_{0},Y_{1},Y_{2,}\dots,Y_{t},\dots) & = & (X_{0},X_{1}-\frac{5}{6}X_{0},X_{2}-\frac{5}{6}X_{1}+\frac{1}{6}X_{0},\dots,X_{t}-\frac{5}{6}X_{t-1}+\frac{1}{6}X_{t-2},\dots)\\
 & = & (\epsilon_{0},\epsilon_{1}-\frac{1}{4}\epsilon_{0},\epsilon_{2}-\frac{1}{4}\epsilon_{1}-\frac{1}{8}\epsilon_{0},\dots,\epsilon_{t}-\frac{1}{4}\epsilon_{t-1}-\frac{1}{8}\epsilon_{t-2},\dots)\,.
\end{eqnarray*}
Par convention, $X_{-k}=\epsilon_{-k}=0$ pour tout $k$ dans $\N^{*}$.
Et donc~:
\[
X_{0}=X_{0}-\frac{5}{6}X_{-1}+\frac{1}{6}X_{-2}\,,\,X_{1}=X_{1}-\frac{5}{6}X_{0}+\frac{1}{6}X_{-1}\,.
\]
Calculons maintenant
\[
(Y_{0},Y_{1}+\frac{1}{2}Y_{0},Y_{2}+\frac{1}{2}Y_{1}+\frac{1}{2^{2}}Y_{2},\dots,Y_{t}+\frac{1}{2}Y_{t-1}+\dots+\frac{1}{2^{t}}Y_{0},\dots)\,.
\]
Le terme général de cette suite est 
\[
Y_{t}+\frac{1}{2}Y_{t-1}-\dots+\frac{1}{2^{t}}Y_{0}=\sum_{k=0}^{+\infty}\frac{1}{2^{k}}Y_{t-k}\,,
\]
puisque les termes de la somme sont nuls à parti d’un certain rang.
Donc, pour tout $t\geq0$~:
\begin{eqnarray*}
Y_{t}+\frac{1}{2}Y_{t-1}-\dots+\frac{1}{2^{t}}Y_{0} & = & \sum_{k=0}^{+\infty}\frac{1}{2^{k}}\left(X_{t-k}-\frac{5}{6}X_{t-k-1}+\frac{1}{6}X_{t-k-2}\right)\\
 & = & X_{t}+\left(-\frac{5}{6}X_{t-1}+\frac{1}{2}X_{t-1}\right)+\sum_{j=2}^{+\infty}X_{t-j}\left(\frac{1}{2^{j}}-\frac{1}{2^{j-1}}\frac{5}{6}+\frac{1}{2^{j-2}}\frac{1}{6}\right)\\
 & = & X_{t}-\frac{1}{3}X_{t-1}+\sum_{j=2}^{+\infty}X_{t-j}\frac{1}{2^{j}}A(2)\\
 & = & X_{t}-\frac{1}{3}X_{t-1}\,.
\end{eqnarray*}
De même~:
\begin{eqnarray*}
Y_{t}+\frac{1}{2}Y_{t-1}-\dots+\frac{1}{2^{t}}Y_{0} & = & \sum_{k=0}^{+\infty}\frac{1}{2^{k}}\left(\epsilon_{t-k}-\frac{1}{4}\epsilon_{t-k-1}-\frac{1}{8}\epsilon_{t-k-2}\right)\\
 & = & \epsilon_{t}+\left(-\frac{1}{4}\epsilon_{t-1}+\frac{1}{2}\epsilon_{t-1}\right)+\sum_{j=2}^{+\infty}\epsilon_{t-j}\left(\frac{1}{2^{j}}-\frac{1}{2^{j-1}}\frac{1}{4}-\frac{1}{2^{j-2}}\frac{1}{8}\right)\\
 & = & \epsilon_{t}+\frac{1}{4}\epsilon_{t-1}+\sum_{j=2}^{+\infty}\epsilon_{t-j}\frac{1}{2^{j}}B(2)\\
 & = & \epsilon_{t}+\frac{1}{4}\epsilon_{t-1}\,.
\end{eqnarray*}
Nous avons donc la relation~:
\[
X_{t}-\frac{1}{3}X_{t-1}=\epsilon_{t}+\frac{1}{4}\epsilon_{t-1}\,,\,\mbox{pour tout }t\in\N^{*}\,.
\]
\end{example}

\section{Tableau des propriétés}

\begin{table}[h]
\begin{tabular}{|c|c|c|c|}
\hline 
Modèle & $MA(q)$ & $AR(p)$ & $ARMA(p,q)$\tabularnewline
\hline 
\hline 
auto-corrélation & $\rho(h)=0$ si $h>q$ & $\rho(h)\underset{h\rightarrow+\infty}{\longrightarrow}0$ & $\rho(h)\underset{h\rightarrow+\infty}{\longrightarrow}0$\tabularnewline
\hline 
auto-corrélation partielle & $r(h)\underset{h\rightarrow+\infty}{\longrightarrow}0$ & $r(h)=0$ si $h>p$ & $r(h)\underset{h\rightarrow+\infty}{\longrightarrow}0$\tabularnewline
\hline 
\end{tabular}

\caption{Tableau des propriétés}
\end{table}

Voir à la fin du chapitre pour des illustrations de ce tableau (pages
tirées du polycopié \cite{jacques}). 

Ces propriétés servent à identifier la nature des séries temporelles.
Sous \texttt{R}, on utilisera les fonctions \texttt{acf}, \texttt{pacf
}qui tracent, respectivement, les $\widehat{\rho}(h)$ et les $\widehat{r}(h)$
(les auto-corrélations empiriques et les auto-corrélations partielles
empiriques). Le logiciel trace en plus un niveau bleu horizontal en
$y=m_{\alpha}$ tel que pour tout $h$, $\p(|\widehat{\rho}(h)|\geq m_{\alpha}|\rho(h)=0)=\alpha$
(en général, le niveau $\alpha$ est fixé à $0,05$) (la situation
est la même pour les auto-corrélations partielles empiriques). On
peut fixer $\alpha$ en ajoutant l’option : \texttt{acf(\dots ,ci=alpha)}.
Un $\widehat{\rho}(h)$ sous la courbe bleue est donc non significatif
(au niveau $\alpha$). Le raisonnement est le suivant : on suppose
$\rho(h)=0$, si $|\widehat{\rho}(h)|<m_{\alpha}$, on considère qu’il
n’est pas nécessaire de revenir sur cette hypothèse de départ. La
probabilité de rejeter à tort l’hypothèse " nulle " ($\rho(h)=0$)
est $\alpha$. 

Mais quand on veut utiliser le tableau ci-dessus, on cherche à savoir
à partir de quel indice les $\rho(h)$ sont nuls (par exemple), pas
si l’un d’eux est nul\footnote{Voir \url{https://xkcd.com/882/} sur le problème des tests multiples.}.
Si on s’intéresse à la nullité de $(\rho(n),\rho(n+1),\dots,\rho(n+l-1))$,
on pourrait vouloir trouver $\beta$ tel que 
\[
\p(\exists h\in\{n,n+1,\dots,n+l-1\}\mbox{ , }\widehat{\rho}(h)\geq m_{\beta}|(\rho(n),\dots,\rho(n+l-1))=(0,\dots,0))\leq\alpha\mbox{ .}
\]
Les $\widehat{\rho}(h)$ sont supposés indépendants donc la probabilité
ci-dessus est 
\begin{eqnarray*}
1-\p(\forall h,|\widehat{\rho}(h)|<m_{\beta}|\dots) & = & 1-(1-\beta)^{l}\\
 & = & l\beta+o(\beta)
\end{eqnarray*}
(quand $\beta$ est petit). Donc on prend, en général, $\beta=\alpha/l$. 

Si on appelle les fonctions \texttt{acf} ou \texttt{pacf} sans préciser
le \texttt{lag.max}, le logiciel le fixe par défaut à $10\log_{10}(n)$
(où $n$ est la longueur de la série). La raison est que l’on ne veut
pas prendre le \texttt{lag.max }trop grand parce que les moyennes
empiriques ne convergent pas bien pour $h$ grand. On conseille en
général de prendre $n\geq50$ et $h\leq n/4$ (voir \cite{brockwell-davis-2002},
p. 60 et \cite{box-jenkins-reinsel-2008}, p. 32).


\section{Les modèles des séries non-stationnaires}

N’importe quand une série n’a pas moyenne constant, elle est non-stationnaire. Pour une série d’être stationnaire il faut que son moyen et sa covariance ne dépend pas au temps $t$. Nous commençons par quelques examples des processus non-stationaires. Dans toutes les examples essayez vous-mêmes de calculer l'espérance,  covariance etc pour devenir à l'aise avec les sommes infinies et calculs. 
\begin{example}
	\label{Example 1} Soit un processus $X_t, \E [X_t]=0$ et une fonction de moyenne non-constant $\mu_t, \E[\mu_t]=c(t)$
	\begin{equation*}
		Y_t=\mu_t+X_t
	\end{equation*}
	est une série non-stationnaire. On peut aller plus loin dans cette “catégorie” des modèles non-stationnaires en écrivant l’example à la forme 
	\begin{equation}
	\label{eq: lin trend}
		Y_t = a+\delta t + \psi(L)e_t 
	\end{equation}
	$\psi(L)$ comme à \ref{psiL} , une partie de la série a une tendance linaire et l’autre est stationnaire. \\
	\textbf{Devoir 1 }\\
	Calculer : l’espérance, la variance, covariance et la correlation de la processus 
	\begin{equation*}
		Y_t=a+\delta t + e_t
	\end{equation*}
\end{example}

\begin{example}
Soit un processus pour lequel le condition de la stationnarité est violé. 
	\begin{equation*}
		Y_t =3 Y_{t-1} + e_t
	\end{equation*}
	En transformant la série en fonction des bruits précédents
	\begin{equation*}
			Y_t=e_t+3e_{t-1}+3^2e_{t-2}+\cdots+3^{k-1}e_1+ 3^{k}Y_0
	\end{equation*}
On remarque que les coefficients ne se tend pas vers zéro.  
\end{example}

\begin{example}{Marche Aléatoire avec derive}
\label{Marche Aléatoire }
\begin{equation}
	Y_t=Y_{t-1} +\delta+e_t
\end{equation}
Ici, $\delta\in \R$ est la dérive, si $\delta=0$ on reviens à la cas classique d’une marche aléatoire.  	\\
\textbf{Devoir 2 }\\
Calculer : l’espérance, la variance, la covariance et la correlation du marche aléatoire avec dérive. Verifier qu’il s’agit d’un processus non-stationnaire.   
\end{example}

Toutes les examples montrent différents cas de non-stationnarité et toujours ) En gross, on pourrais identifier deux categories de pathologie: soit que il y a  
En général, dans une série non-stationnaire, l’influence des valeurs precedents augmente. On peut appeler cette comportement explosif. 


\subsection{Les processus ARIMA}
Dans cette section nous nous occupons avec les cas ou la source de la non-stationnarité est la violation des conditions du polynôme characteristic comme il était montré par les examples précédents. Pour un processus $AR(1)$  il faut que la racine du polynôme soit entre $1,-1$ strictement.
On peut écrire l’example 3 en utilisant l’opérateur de décalage du temps (lag) comme 
\begin{equation*}
	(1-L)Y_t=\delta + e_t
\end{equation*}
Ici, il est clair qu’il s'agit d' une racine unitaire, et alors ce n’est pas possible de tourner le modèle en forme rétroactif convergante. Mais si on prendre les premières difference on voit que 
\begin{equation}
	\Delta Y_t=Y_t-Y_{t-1}=\delta+ e_t
\end{equation}
qui donne une série stationnaire. Le même argument s’applique aussi à la cas générale de la forme
\begin{equation}
\label{eq: genMA unit root}
	(1-L)Y_t=\delta + \psi(L)e_t
\end{equation}
et on appelle cette catégorie des processus non-stationnaires, \textit{processus de racine unitaire (unit root processes) }  et le processus qui sort des differences, \textit{processus intégré d’ordre 1}, I(1)    

On peut aussi imaginer un processus ARMA qui contient les racines unitaires en multiple fois. 
\begin{equation}
	\label{eq: ARMA Unit Root}
	(1-L)^k(1-R_1L)(1-R_2L)\cdots (1-R_{p-k}L)Y_t=\psi(L)e_t
\end{equation}
ou $R_1,\cdots R_{p-k}$ sont les reciprocals des racines différents, et la multitude de racine unitaire est $k$. On s’appelle alors \ref{eq: ARMA Unit Root} un processus $ARIMA(p,d,q)$ avec $d=k$ et $q$ dépendant à la représentation du $\psi(L)$ 

Une autre catégorie de pathologie qui peut rendre une série non-stationnaire est la tendance linéaire, comme à l'example \ref{Example 1} et l'equation \ref{eq: lin trend}. 

Pour soulager la non-stationnarité on prend les différences en applicant l'opérateur des difference $\Delta$ autant des fois il est nécessaire pour rendre la série stationnaire. Il faut prêter attention d'être économe en applicant l'opérateur car trop des differences créent des problèmes au partie MA, la raison précis est hors du sujet de ces notes  Pour la même raison, si on  soupçonne une tendance linaire, il est conseillé d'essayer l'enlever avant prendre les premiers différences. 



\subsection{ IMA(1,1) }
 \begin{equation}
 	\label{eq: IMA}
 	Y_t=Y_{t-1}+e_t-\theta e_{t-1}
 \end{equation}
 Ici, c’est un modèle avec une racine unitaire et un lag au bruit, alors  $ARIMA(0,1,1)$ Pour le rendre stationnaire nous prenons les premières differences
 \begin{equation*}
 W_t=\Delta Y_t=e_t-\theta e_{t-1}
 \end{equation*}
 Maintenant, $W_t$ est un processus $MA(1)$  que nous avons déjà étudié


Équation  \ref{eq: IMA} pourrait être transformé en forme rétroactif 
\begin{equation}
	\label{eq: IMA retro}
	Y_t= e_t+(1-\theta)e_{t-1}+\cdots +(1-\theta)e_{t-k+1}+\theta e_{t-k}
\end{equation}
 \textbf{Devoir}\\
 Avec l’aide de \ref{eq: IMA retro} montrez
 \begin{itemize}
 	\item $\E [Y_t]= 0$
 	\item $Var(Y_t)=[1+\theta^2 + (1-\theta)^2(t+k)]\sigma_e^2$
 	\item $Cov(Y_t,Y_{t-h})= 1+\theta^2 + (1-\theta)^2(t+k-h) $
 \end{itemize}
 \subsection{ ARI(1,1)}
 \begin{equation}
 	\label{eq: ARI}
 	Y_t=(1-\phi)Y_{t-1} - \phi Y_{t-2}+e_t
 \end{equation}
 ou 
 \begin{equation*}
 	W_t=\Delta Y_t =\phi W_{t-1}+e_t
 \end{equation*}
 Maintenant, $W_t$ est un processus $AR(1)$ comme aux chapitres précédents.
 Si nous voulons écrire le polynôme characteristic pour cette processus, nous arrivons à la formule 
 \begin{equation*}
 	\frac{1}{ (1-z)(1-\phi z)} =1+\psi_1z+\psi_2z^2+\cdots
 \end{equation*}
 ou 
 \begin{equation}
 	 	\label{eq: inversion ARI}
 	 	(1-(1+\phi)z+\phi z^2)(1+\psi_1z+\psi_2z^2+\cdots)=1
 \end{equation}
pour définir les $\psi_i$  nous arrivons au system 
\begin{align*}
	&-(1+\phi)+\psi_1=0\\
	&\phi-(1+\phi)\psi_1+\psi_2=0
\end{align*}   
et 
\begin{equation}
\label{ARI coef}
	\psi_k=(1+\phi)\psi_{k-1}-\phi\psi_{k-2}, \quad \psi_0=1
\end{equation}
 
 \textbf{Devoir}
 Calculer la variance, la covariance et la correlation du processus ARI(1,1) \ref{eq: ARI}
 




 \section {Specification du Modèle, la méthodologie Box-Jenkins}

 Après l’étude théorique des modèles ARMA et ARIMA maintenant nous nous occupons avec les questions pratiques. 
 \begin{itemize}
 	\item Comment modéliser les donnes comme un processus $ARIMA(p,d,q)$? 
 	\item Est-ce que la série est stationnaire?  
 	\item Comment choisir $p,d,q$?
 \end{itemize}
 
 Pour répondre à ces questions à la manière pratique et pragmatique, Box et Jenkins\footnote{G. Box, G. Jenkins, \textit{Time Series Analysis: forecasting and control}, 1970 } ont développé une methodology en 4 étapes: 
\begin{enumerate}
	\item Identification 
	\item Estimation 
	\item Diagnostique
	\item Prévision
\end{enumerate}


 \subsection{ Identification} 
 	 En cet étape on s’occupe de determiner l’ordre du modèle, et le comportement stationnaire (ou non) et saisonnier (ou non). Il y a deux approches pour traiter cette partie
\begin{itemize}
	\item  L’approche graphique: On plot la série, les ACF, PACF en cas que ACF tend ver zéro très lentement ou pas du tout on soupçonne nonstationnarité. Box et Jenkins conseillent à prendre les premières differences et refaire les graphs en cas qu’ils indiquent stationnarité.Sinon, on reprendre les différences jusqu’à le graphe a l’air stationnaire. 


Pour determiner l’order du modèle $p,q$ il y a quelque règles empiriques à partir de graph de ACF 
\begin{enumerate}
\item Décroissance exponentielle vers zéro : Modèle AR (PACF indique l’ordre p)
\item Oscillations amortis décroissants rapidement vers zéro: AR  (PACF indique l’ordre p)
\item Quelque peu pics, le reste nulle: MA (l’ordre est le nombre des pics)
\item  Décroissance exponentielle après quelque pas de temps (lags)
\item pattern régulière en écarts de temps : saisonnalité 
\end{enumerate}
Un remarque général, ACF nous aide à determiner l’order MA du modèle et PACF l’ordre AR. 
	\item L'approche statistique. Il y a quelque test statistiques que on peut essayer pour reconnaitre la stationnarité ou non de notre modèle, voire la prochaine section. Une fois que la question de stationnarité serais répondu on tourne sur la determination d'ordre AR ou MA par la vois statistique. 
On choisit entre plusieurs modèles en regardant~:
\begin{itemize}
\item l’ajustement à la série de données,
\item la complexité du modèle (il est plus facile d’estimer un nombre réduit
de paramètres).
\end{itemize}
Pour concilier ces deux critères, on minimise une des deux quantités\index{AIC}\index{BIC}
suivantes 
\begin{eqnarray*}
AIC & = & -2\log(L(\theta))+2\nu\,,\\
BIC & = & -2\log(L(\theta))+n\log(\nu)\,,
\end{eqnarray*}
où $\nu$ est le nombre de paramètres ($n=p+q$ si'l s'agit d'un ARMA sans constant ou $n=p+q+1$),  $\theta$ est un vecteur contenant
les paramètres, $n$ est le nombre d’observations, $L$ est la vraisemblance
(dans laquelle on a omis les observations).

AIC (Akaike's Information Criterion) est un estimateur d'espérance  de la divergence de Kullback-Leibler entre le modèle estimé et le vrai modèle. Soit $p(y_1,...,y_n)$ la vrai densité de probabilité de $Y_1,...,Y_n$ et soit $q_\theta(y_1,...,y_n)$  la densité sous le modèle de paramètre $\theta$ la divergence Kullback-Leibler de $q_\theta$ à $p$ est donner par la formule suivante
\begin{equation*}
	D(p,q_\theta) =\int_{-\infty}^\infty \int_{-\infty}^\infty.... \int_{-\infty}^\infty p(y_1,...,y_n)\log\biggl(\frac{p(y_1,...,y_n)}{q_\theta(y_1,...,y_n)} \biggr)dy_1...dy_n
\end{equation*} 
On veut estimer l'espérance $\E [D(p,q_{\hat{ \theta}})]$ ou $\hat{ \theta}$ est l'estimateur du vecteur des paramètres, qui sort la formule du debut. 
 
\begin{example}
Le calcul des quantités $AIC$ et $BIC$ se fait facilement en \texttt{R}.
Calculons une vraisemblance dans un cas simple. 

On suppose $X_{t}=at+b+\epsilon_{t}$ avec des $\epsilon_{t}$ i.i.d.
de loi $\mathcal{N}(0;1)$. Les paramètres du modèle sont $a$ et
$b$. On dispose d’observations $x_{1},\dots,x_{n}$. La densité de
$(X_{1},\dots,X_{n})$ est la fonction
\[
(u_{1},\dots,u_{n})\mapsto\prod_{t=1}^{n}\left\{ \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}(u_{t}-at-b)^{2}\right)\right\} \,.
\]
La vraisemblance est donc 
\[
(a’,b’)\mapsto L(a’,b’)=L((a’,b’);(x_{1},\dots,x_{n}))=\prod_{t=1}^{n}\left\{ \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}(x_{t}-a’t-b’)^{2}\right)\right\} \,.
\]
Si on veur estimer $(a,b)$ à partir de $(x_{1},\dots,x_{n})$, l’estimateur
du maximum de vraisemblance\index{Maximum de vraisemblance} est 
\[
(\widehat{a},\widehat{b})=\argmax_{a’,b’}L(a’,b’)\,.
\]
 
\end{example}


\end{itemize}
Pour la saisonnalité, nous allons apprendre plus au chapitre 5
 \subsection{ Estimation du modèle}  

On suppose que $X_{t}=a_{1}X_{t-1}+a_{2}X_{t-1}+\epsilon_{t}$ (avec
des $\epsilon_{t}$ bruits blancs centrés, de variance $\sigma^{2}$)
et que $(X_{t})$ est stationnaire. On calcule les auto-covariances~:
\[
\sigma(1)=\frac{a_{1}}{1-a_{2}}\sigma(0)\,,\,\sigma(2)=a_{1}\sigma(1)+a_{2}\sigma(0)\,.
\]
D’où
\begin{eqnarray*}
a_{1} & = & \frac{\sigma(1)}{\sigma(0)}\times\frac{\sigma(0)^{2}-\sigma(0)\sigma(1)}{\sigma(0)^{2}-\sigma(1)^{2}}\,,\\
a_{2} & = & \frac{\sigma(0)\sigma(2)-\sigma(1)^{2}}{\sigma(0)^{2}-\sigma(1)^{2}}\,.
\end{eqnarray*}
On peut donc estimer $a_{1}$, $a_{2}$ en remplaçant $\sigma(0)$,
$\sigma(1)$, $\sigma(2)$ par leurs estimateurs empiriques dans les
formules ci-dessus. Puisque $\sigma(0)=\sigma^{2}+a_{1}\sigma(1)+a_{2}\sigma(2)$,
on peut aussi estimer $\sigma^{2}$. 

Dans le cas général, on estimer par maximum de vraisemblance (voir
cours de statistiques et l’exemple ci-dessous).

\subsection{Diagnostique du modèle}.  Ici, nous nous intéressons principalement au comportement des résidus du modèle. Quels sont le propriétés statistiques des résidus? Hypothèse de normalité, bruit blanc etc. Box-Pierce ou Ljung-Box sont les utiles principales ici. Deux remarques importants 
 	\begin{enumerate}
 	\item En testant la correlation des résidus, il n’est possible que de dévoiler un modèle sousparametrisé. Il y a rien à dire pour un modèle surparametrisé. 
 	\item Autocorrelation des résidus est possible de faire les autres test statistiques donner les faux résultats. 
 	\end{enumerate}
\subsection{Prévision } Si le modèle est un processus $ARMA$, la prédiction pour $X_{n+h}$,
sachant $X_{1},\dots,X_{n}$ est 
\[
\widehat{X}_{n,h}=c_{1}X_{1}+\dots+c_{n}X_{n}\,,
\]
où les coefficients sont choisis de manière à minimiser l’erreur quadratique
\[
\E((X_{n+h}-c_{1}X_{1}-\dots-c_{n}X_{n})^{2})\,.
\]

\begin{prop}
Ce choix de $\widehat{X}_{n,h}$ entraîne l’égalité
\[
\widehat{X}_{n,h}=\E(X_{n+h}|X_{1},\dots,X_{n})\,.
\]
\end{prop}
Ce qui n’est pas surprenant si on se rappelle que l’espérance conditionnelle
de $X_{n+h}$ sachant $X_{1},\dots,X_{n}$ est la projection orthogonale
de $X_{n+h}$ sur $\sigma(X_{1},\dots,X_{n})$ dans l’espace des variables
$L^{2}$ (muni de la norme $L^{2}$). 
\begin{prop}
L’erreur de prévision à l’horizon $1$ ($X_{n+1}-\widehat{X}_{n,1}$)
est le bruit d’innovation $\epsilon_{n+1}$.\\
La variance de l’erreur de prévision ($\E((X_{n+h}-\widehat{X}_{n,h})^{2})$)
est croissante avec $h$ et tend vers $\var(X_{1})$ quand $h\rightarrow+\infty$
(on rappelle que le procesus $(X_{t})$ est supposé stationnaire).
\end{prop}
Puisque les $\epsilon_{t}$ sont gaussiens, les $X_{t}$, $\widehat{X}_{n,h}$
et $\widehat{X}_{n,h}-X_{n+h}$ sont aussi gaussiens (nous sautons
une petite démonstration). Ceci permet de construire facilement des
intervalles de confiance\index{Intervalle de confiance} (c’est inclus
dans \texttt{R}). Nous faisons ici un rappel dans un cas simple. Supposons
$\widehat{X}_{n,h}-X_{n+h}\sim\mathcal{N}(0;\sigma^{2})$. Soit $\alpha=0,01$.
Nous cherchons $\Delta$ tel que
\[
\p(X_{n+h}\in[\widehat{X}_{n,h}-\Delta;\widehat{X}_{n,h}+\Delta])\geq1-\alpha
\]
(ici, toutes les probabilités sont conditionnelles à $X_{1},\dots,X_{n}$).
Nous calculons
\begin{eqnarray*}
\p(X_{n+h}\in[\widehat{X}_{n,h}-\Delta;\widehat{X}_{n,h}+\Delta]) & = & \p(|X_{n+h}-\widehat{X}_{n,h}|\leq\Delta)\\
 & = & \p\left(\frac{|X_{n+h}-\widehat{X}_{n,h}|}{\sigma}\leq\frac{\Delta}{\sigma}\right)\\
\mbox{(en utilisant les symétries de la gaussienne)} & = & 1-2\p\left(\frac{X_{n+h}-\widehat{X}_{n,h}}{\sigma}\geq\frac{\Delta}{\sigma}\right)\,.
\end{eqnarray*}
Nous voulons donc $\Delta$ tel que 
\[
\p\left(\frac{X_{n+h}-\widehat{X}_{n,h}}{\sigma}\geq\frac{\Delta}{\sigma}\right)\leq\frac{\alpha}{2}=0,005\,.
\]
Puisque $(X_{n,h}-\widehat{X}_{n,h})/\sigma$ est de loi $\mathcal{N}(0;1)$,
on lit dans une table de la loi normale qu’il suffit de prendre $\Delta/\sigma=2,58$
pour que l’inégalité ci-dessus soit vérifiée. 


\subsection{Tests statistiques pour la stationnarité}
Comme nous avons vu si le polynôme d’une série contient une racine unitaire, la série n’est pas stationnaire.  Dès premières étapes de la méthodologie Box-Jenkins on peut soupçonner la non-stationnarité par les graphes mais ici notre but c’est de developer des tests statistiques pour tester rigoureusement les séries. Souvent il y a deux cas de non-stationnarité et ce n'est pas toujours facile à les distinguer, trend-stationnaire (c’est-à-dire la série n’est pas stationnaire par rapporte de definition mais elle est stationnaire autour une tendance deterministic) et racine unitaire non-stationnaire. Les tests qui suivent sont développés pour la deuxième catégorie. 
 \subsubsection{Augumented Dickey-Fuller test de racine unitaire}
\begin{description}
\item{\textbf{Dickey-Fuller test}}
\begin{equation}
	Y_t=aY_{t-1}+e_t
\end{equation}
Avec les hypothèse:
\begin{align*}
	&H_0: a=1\\
	&H_1: a<1
\end{align*}
sous l’hypothèse $H_0$ nous avons un marche aléatoire. DF-test est le t-test de $H_0$ et on construit la statistique pour l’estimateur $\hat a $ de $a$
\begin{equation*}
	\hat\tau=\frac{\hat a -1}{se(\hat a)}
\end{equation*}
$\hat\tau$ ne suis pas Gaussian et la distribution analytic pourrait être rétablie par Hamilton p489 et table B6
\textit{Remarques}
\begin{enumerate}
	\item $e_t$ doit être bruit blanc
	\item La distribution de $\hat\tau$ reste la même aussi pour un modèle ARMA  
\end{enumerate}
\item{\textbf{Augmented Dickey-Fuller test}}
\begin{equation}
	\begin{split}
		&Y_t=aY_{t-1}+X_t\\
		&X_t=\phi_1X_{t-1}+\phi_2X_{t-2}+\cdots+\phi_k X_{t-k}+e_t
	\end{split}
\end{equation}

Avec les hypothèse:
\begin{align*}
	&H_0: a=1\\
	&H_1: a<1
\end{align*}
Sous $H_0$ nous avons $\Delta Y_t$ d’être un processus stationnaire $AR(k)$ 
\begin{equation*}
	Y_t-Y_{t-1}=X_t
\end{equation*}
et $Y_t$ un $AR(k+1)$ avec polynôme caractéristique
\begin{equation*}
	(1-az)(1-\phi_1z-\cdots\phi_k z^k)=0
\end{equation*}
\item{\textbf{Augmented Dickey-Fuller avec contant et dérive}}

\begin{equation}
	\begin{split}
		&Y_t=\mu +\delta t+ aY_{t-1}+X_t\\
		&X_t=\phi_1X_{t-1}+\phi_2X_{t-2}+\cdots+\phi_k X_{t-k}+e_t
	\end{split}
\end{equation}

Avec les hypothèse:
\begin{align*}
	&H_0: a=1\\
	&H_1: a<1
\end{align*}
Sous $H_0$
\begin{equation*}
	\Delta Y_t= X_t+\delta
\end{equation*}

\end{description}

\textit{Remarques}

\begin{enumerate}
	\item C’est ADF avec dérive et constant que $R$ utilise comme method standard pour la command \texttt{adf.test()} dans la librairie \texttt{tseries}. 
	\item $k$ paramètre en $R$ est précisément l’ordre du modèle AR pour $X_t$
	\item Si on met $k=0$ en $R$ elle comptera le test DF 
	\item La distribution pour la statistique $\hat\tau$ de chaque modèle change mais il sont disponibles analytiquement à Hamilton p.502 et p.528-529
\end{enumerate}
Il y a encore les tests Philips-Perron et KPSS qui pourront être utilisés en complement de ADF pour verifier la stationnarité ou non d’une série. 

\begin{example}{Un bruit blanc}
On fait la simulation de 1000 bruit blanc comme représenté à l'image 

\begin{figure}[h]
	\includegraphics[width=0.4\linewidth]{White.jpeg}
	\caption{Bruit Blanc}
\end{figure}


	Pour tester notre série pour une racine unitaire on frappe 
	
	Qui donne \textbf{p-value <0.01} pour un nombre des lags 7 et alors on rejette l'hypothèse zero de non-stationnarité 
	
Par contre pour une série dérivée par un bruit blanc qui est donne par la commende $R $ 
\begin{figure}[h]
\includegraphics[width=0.4\linewidth]{Diffinv_WN}
	\caption{L'inverse des premiers differences}
\end{figure} 


	le test ADF donne \textbf{p-value =0.7258} pour un nombre des lags 9 et alors on ne peut pas rejeter l'hypothèse zéro de la non-stationnarité.\\ 
Codes:  
	\begin{verbatim} 
	x <- rnorm(1000)
	plot(x)
	adf.test(x)
	 \end{verbatim}
	 et 
	\begin{verbatim} 
	y <- diffinv(x) 
	plot(y)
	adf.test(y)
	 \end{verbatim} 


\end{example}



   
 \subsection{Deux examples détaillés} 	
 
 \begin{description}
 	\item[Deere] 
 	 Le fichier de données nommé « deere3 » contient 57 mesures consécutives enregistrées à partir d'une machine-outil complexe chez Deere \& Co. Les valeurs données sont des écarts par rapport à une valeur cible en unités de dix millionièmes de pouce.  Le processus utilise un mécanisme de contrôle qui réinitialise certains des paramètres de la machine-outil en fonction de l'ampleur de l'écart par rapport à la cible du dernier article produit
 
 Pour étudier cette série nous suivons les pas de la méthodologie Box-Jenkins. 
 \begin{description}
 	\item[Identification] 
 	 
 \begin{enumerate}
 \item \textbf{Stationnarité} 
 On voit le graph et aussi les fonctions ACF et PACF
 \begin{figure}[h]
 	\includegraphics[width=0.4\linewidth]{deere3_plot.jpeg }
 \end{figure}
 
 \begin{figure}[h]
 	\includegraphics[width=0.4\linewidth]{acf_deere3.jpeg }
 	\includegraphics[width=0.4\linewidth]{pacf_deere3.jpeg }
 \end{figure}
  
  
  Graphiquement, la série a l'air stationnaire, les autocorrelations tendent vers zéro rapidement et seulement le premier lag est important des autocorrelations partielles (c'est qui indique un modèle AR1). Pour le vérifier on utilise encore le test ADF qui nous donne $\textbf{p-value} < 0.01$ au niveau 3-lags et alors nous pouvons verifier la stationnarité de la série. 
   
  
 \item \textbf{ Determiner l'ordre AR et ou MA}

 Pour choisir le modèle ARMA, on voit les ACF, PACF et on teste progressivement de AR1, jusqu'à AR4 et les combinaisons avec MA1, .. pour trouver que celui qui maximise le critère AIC, c'est ARMA(2,1)

 \end{enumerate}
 	 

 \end{description}
\textbf{Pour resumer cette analyse, la série initial deere3 pourrait être décrit comme un modèle ARMA(2,1)}
 	\item[Wages]
 	 
 
 
 Le fichier de données nommé "wages" contient mensuel valeurs de moyenne salaire par heure pour les travailleurs aux Etats-Unis dans l'industrie textile de juillet 1981 jusqu'a juin 1987 
 
 \begin{description}
 	\item[Identification]
 	  On commence par une plot de la série 
 	
 \begin{figure}[h]
 	\includegraphics[width=0.4\linewidth]{wages_plot.jpeg}
 \end{figure}
 
 Par le graph on peut soupçonner une tendance linéaire avec le temps, on essaie de l'enlever et afficher les résidus de la regression. 
 
 \begin{figure}[h]
 	\includegraphics[width=0.4\linewidth]{reg_resids.jpeg}
 \end{figure}
 
 les résidus semblent un peu pathologiques et on affiche aussi le ACF pour prendre plus d'informations. 
 
 \begin{figure}[h]
 	\includegraphics[width=0.4\linewidth]{acf_reg_resids.jpeg}
 \end{figure}
 
 On soupçonne une non-stationnarité persistent et on prendre le premieres difference pour rendre les résidus stationnaires. 
 
 \begin{figure}[h]
 	\includegraphics[width=0.4\linewidth]{diff_reg_resids.jpeg}
	\includegraphics[width=0.4\linewidth]{acf_diff_reg_resids.jpeg} 
 \end{figure}
 
 maintenant ils ont l'air stationnaire. On peut verifier les résultats par la voie statistique en faisant à chaque étape un test Box-Pierce sur les résidus. Pour le premier après la regression  on rejet l'hypothèse zero d'être bruit blanc et on peut verifier l'existence de non stationnarité par un test ADF avent prendre le premiers difference.  
 
 
 	
 	
 	\item[Ordre du model AR, MA ] on voit sur le dernier plot de la série stationnaire un seul lag statistiquement significative et alors on decide de commencer avec un model AR1
 	

 \end{description}

 \textbf{Pour resumer cette analyse, la série initial wage pourrait être modélisé comme un processus ARIMA(1,1,0) avec tendance linaire.} 
 
 
\item[ Codes ] Pour cette code on a besoin de la librairie \textbf{tseries}. 

 
	\begin{verbatim} 
	data = read.csv("deere3.csv", col.names ='x' )

deere3= ts(data)
plot(deere3)
acf(deere3)
pacf(deere3)
library(tseries)
adf.test(deere3)
outAR1 = arima(deere3, order = c(1,0,0)) 
outAR2 = arima(deere3, order = c(2,0,0))
outAR3 = arima(deere3, order = c(3,0,0))
outAR4 = arima(deere3, order = c(4,0,0))
outARMA21 = arima(deere3, order = c(2,0,1))
Box.test(outAR1$resid,lag=1)
Box.test(outAR2$resid,lag=2)
Box.test(outAR3$resid,lag=3)
Box.test(outAR4$resid,lag=4)
prediction = predict(outAR1, n.ahead=5)
plot(deere3)
points( prediction$pred, type = "l", col = 2 )

	 \end{verbatim}
	 et 
	\begin{verbatim} 
	
data2 = read.csv("wages.csv", col.names = "Wages")
wages = ts(data2)
plot(wages)
t=1:72
reg = lm(wages~t)
plot(reg$fitted.values,t, type = "l")
plot(reg$residuals, type = "l")
acf(reg$residuals)
Box.test(reg$residuals, lag = 1)
adf.test(reg$residuals)
wagesRes = reg$residuals
dwagesRes = diff(wagesRes)
plot(dwagesRes)
acf(dwagesRes)
out1= arima(dwagesRes, order = c(1,0,0))
Box.test(out1$residuals, lag = 1)
	 \end{verbatim} 


 \end{description}


\section{Processus non stationnaires~: ARIMA et SARIMA \index{ARIMA}\index{SARIMA}}

On veut revenir à la série d’origine une fois que l’on a étudié la
partie stationnaire
\begin{example}
Nous disposons d’une série temporelle $(x_{1},\dots,x_{n})$ qui a
une saisonnalité de période $12$. On étudie $y_{t}=x_{t}-x_{t-12}$
(pour supprimer la saisonnalité). L’ajustement d’un modèle $ARMA$
et les prévisions sont réalisées sur la série $(y_{t})$. On écrit
ensuite les $(x_{t})$ en fonction des $(y_{t})$~:
\begin{eqnarray*}
x_{t} & = & y_{t}+x_{t-12}\\
 & = & y_{t}+y_{t-12}+x_{t-24}\\
 & = & \dots\\
 & = & y_{t}+y_{t-12}+\dots+y_{r+12}+x_{r}\,,
\end{eqnarray*}
avec $r=t\mbox{ modulo }12$ (le reste de la division euclidienne
de $t$ par $12$).
\end{example}
On connaît les $x_{1},\dots,x_{n}$ ($n$ supposé plus grand que $12$),
et donc aussi $y_{1},\dots,y_{n}$. On peut calculer les prévisions
$\widehat{y}_{n,h}$ ($h\geq1$). On en déduit les prévisions $\widehat{x}_{n,h}$.
Par exemple, si $h\in\{1,2,\dots,11\}$, nous utilisons l’égalité
ci-dessus avec $t=n+h$ pour calculer la prévision
\[
\widehat{x}_{n,h}=\widehat{y}_{n,h}+y_{t-12}+\dots+\dots+y_{r+12}+x_{r}\,.
\]


\subsection*{Les processus}

Ce sont des généralisation des processus $ARMA$ aux cas non stationnaires,
avec tendance polynômiale ($ARIMA$) ou avec une saisonnalité ($SARIMA$).
Ce sont les processus directement utilisés par \texttt{R}.
\begin{defn}
Le processus $(X_{t})_{t\geq0}$ est un processus $ARIMA(p,d,q)$
si le processus $Y_{t}=\Delta_{1}^{d}X_{t}$ est une processus $ARMA(p,q)$. 
\end{defn}
Les processus $ARIMA(p,d,q)$ sont donc bien adaptés à l’étude des
séries temporelles présentant une tendance polynômiale de degré $d-1$. 
\begin{defn}
Le processus $(X_{t})_{t\geq0}$ est un processus $SARIMA(p,d,q,T)$
si le processus $Y_{t}=\Delta_{T}\circ\Delta_{1}^{d}X_{t}$ est un
processus $ARMA(p,q)$.
\end{defn}
Les processus $SARIMA(p,d,q,T)$ sont donc bien adaptés à l’étude
des séries temporelles qui présentent une saisonnalité de période
$T$ et qui ont une tendance polynômiale de degré $d-1$.
\begin{rem}
Attention, il existe dans la littérature des définitions de processus
$SARIMA$ plus complexes.
\end{rem}


\subsection{Exercices supplementaires }
\begin{enumerate}
	\item Devoir 1: Calculer la variance de la série: 
	\begin{equation*}
		Y_t=e_t+\psi e_{t-1}+\psi^2 e_{t-2}+...
	\end{equation*}
	\begin{proof}
	\begin{align*}
		Var(Y_t)&= Var(\sum_{i=0}^\infty\psi^ie_{t-i}) \\&=\sum_{i=0}^\infty Var(\psi^i e_{t-i}) + \sum_{\substack{j=0\\j \neq i }}^\infty Cov(\psi^i e_{t-i},\psi^j e_{t-j}) \\ 
		&= \sum_{i=0}^\infty \psi^{2i} Var( e_{t-i})  = \sigma_e \frac{1}{1-\psi^2}
	\end{align*}
l'equation finale viens de la série géométrique de $\psi^2$
	\end{proof}
	\item Calculer la variance de la série 
	\begin{equation*}
		Y_t= e_t +\psi e_{t-1}+ \psi^2 e_{t-2} +...+ \psi^k e_{t-k}
	\end{equation*}
	\begin{proof}
		$$Var(Y_t)= \sigma_e \sum_{i=0}^k \psi^{2i}=\sigma_e \frac{1-\psi^{2(k+1)}}{1-\psi^2}  $$
		Pour arriver à cette dernier equation il faut écrire $$\sum_{i=0}^k \psi^{2i} = \sum_{i=0}^\infty \psi^{2i}-\sum_{i=k+1}^\infty\psi^{2i}= \frac{1}{1-\psi^2}- \psi^{2(k+1)}\sum_{n=0}^\infty \psi^{2n}  $$
	\end{proof}
	
	\item Pour le modèle suivant, calculer l' espérance, la variance, la covariance et la correlation. 
	\begin{equation*}
		\label{ex: ARIMA}
		Y_t = - Y_{t-1}+2 Y_{t-2}+e_t 
	\end{equation*}
	On fait l'hypothèse que $e_t$ est un bruit blanc centré de variance $\sigma_e$
	\begin{proof}
	On va récrire le modèle à la forme des lags pour définir le polynôme characteristic:
		\begin{equation*}
			Y_t = - Y_{t-1}+2 Y_{t-2}+e_t 
		\end{equation*}
		\begin{equation*}
			(-2L^2+L+1)Y_t= e_t
		\end{equation*}
		qui a les racines $r_1=1,\quad r_2=-\frac{1}{2} $.  Il est clair que nous avons une racine unitaire et alors il s'agit d'un modèle genre ARI(1,1,0). 
		\begin{equation*}
			\frac{1}{(1-z)(1+2z)}= \psi_1 z + \psi_2z^2 +... 
		\end{equation*} 
		En utilisant l'equation \eqref{ARI coef} on peut récrire l'equation précédant comme 
		\begin{align*}
			&\psi_k=3\psi_{k-1} -2\psi_{k-2} \quad \psi_0=1 \\
			ou \quad & \psi_k=\frac{1-\phi^{k+1}}{1-\phi}
		\end{align*}
		Par qui on peut calculer facilement l'espérance 
		$$\E[Y_t]=0 $$
		et la variance
		$$Var(Y_t)= \sigma_e \big[ 1+2^2+...+ (\frac{1-2^{t-k}}{-1})^2 \big]$$ 
	\end{proof}
\end{enumerate}

\end{document}
